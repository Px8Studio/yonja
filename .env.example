# Yonca AI Environment Configuration
# Copy this file to .env and customize values
# All variables use YONCA_ prefix

# ===========================================
# Deployment (Two-Axis Model)
# ===========================================
# Axis 1: Environment — WHAT stage of development
YONCA_ENVIRONMENT=development  # development | staging | production

# Axis 2: Infrastructure Mode — WHERE it runs
YONCA_INFRASTRUCTURE_MODE=local  # local | cloud

# Debug mode (auto-enabled in development)
YONCA_DEBUG=false

# DEPRECATED: Use YONCA_ENVIRONMENT + YONCA_INFRASTRUCTURE_MODE instead
# YONCA_DEPLOYMENT_MODE=local  # local | open_source | cloud

# ===========================================
# LangGraph Server (Required for HTTP Mode)
# ===========================================
# When true, LangGraph Dev Server must be running (recommended)
# When false, falls back to in-process execution (legacy direct mode)
YONCA_LANGGRAPH_REQUIRED=true


# ===========================================================
# MCP Server Settings
# ===========================================================

# MCP General
MCP_ENABLED=true

# Weather MCP
WEATHER_MCP_ENABLED=true
WEATHER_MCP_URL=https://openweather.mcp.example.com
WEATHER_API_KEY=your_key

# ZekaLab MCP
ZEKALAB_MCP_ENABLED=true
ZEKALAB_MCP_URL=http://localhost:7777
ZEKALAB_TIMEOUT_MS=2000

# ===========================================
# API Settings
# ===========================================
YONCA_API_HOST=0.0.0.0
YONCA_API_PORT=8000
YONCA_API_WORKERS=4
YONCA_CORS_ORIGINS=["http://localhost:3000", "http://localhost:8501"]

# ===========================================
# LLM Provider Configuration
# ===========================================
# Choose: ollama (local/slow), groq (cloud/ultra-fast), gemini (cloud/fast)
YONCA_LLM_PROVIDER=ollama

# Ollama (Local) - Slow on CPU without GPU!
# Use for offline/low-connectivity scenarios
YONCA_OLLAMA_BASE_URL=http://localhost:11434
YONCA_OLLAMA_MODEL=qwen3:4b
# Alternative Ollama models:
# qwen3:1.7b  - Faster on CPU, lighter
# atllama     - Azerbaijani-tuned LLaMA (GGUF import)

# Groq (Cloud) - Ultra-fast inference, FREE tier!
# Get your free API key at: https://console.groq.com/
# Recommended for demos and development on slow hardware
# YONCA_LLM_PROVIDER=groq
# YONCA_GROQ_API_KEY=gsk_your-api-key-here
# YONCA_GROQ_MODEL=llama-3.1-8b-instant
# Alternative Groq models:
# llama-3.3-70b-versatile - Best quality
# llama3-8b-8192          - Good balance
# mixtral-8x7b-32768      - Large context

# Gemini (Cloud) - Google's fast cloud API
# Get your free API key at: https://ai.google.dev/
# YONCA_LLM_PROVIDER=gemini
# YONCA_GEMINI_API_KEY=AIza-your-api-key-here
# YONCA_GEMINI_MODEL=gemini-2.0-flash-exp

# ===========================================
# Database
# ===========================================
YONCA_DATABASE_URL=sqlite+aiosqlite:///./data/yonca.db
YONCA_DATABASE_POOL_SIZE=10
YONCA_DATABASE_MAX_OVERFLOW=20

# ===========================================
# Redis
# ===========================================
YONCA_REDIS_URL=redis://localhost:6379/0
YONCA_REDIS_MAX_CONNECTIONS=50

# ===========================================
# Security
# ===========================================
YONCA_JWT_SECRET=dev-secret-change-in-production
YONCA_JWT_ALGORITHM=HS256
YONCA_JWT_EXPIRY_HOURS=24

# ===========================================
# Rate Limiting
# ===========================================
YONCA_RATE_LIMIT_REQUESTS_PER_MINUTE=30
YONCA_RATE_LIMIT_BURST=50

# ===========================================
# Observability
# ===========================================
YONCA_LOG_LEVEL=INFO
YONCA_LOG_FORMAT=json  # json | console
YONCA_PROMETHEUS_ENABLED=true

# LangChain/LangGraph Verbose Logging (for debugging agent execution)
# Set to true to see detailed node execution, LLM calls, and state transitions
LANGCHAIN_TRACING_V2=true  # Set to true when debugging agent flow
LANGCHAIN_PROJECT=yonca-dev
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=  # Optional: Only needed for LangSmith cloud tracing

# LangGraph API Log Level (affects langgraph-api server logs)
LANGGRAPH_API_LOG_LEVEL=INFO  # DEBUG for verbose server logs

# ===========================================
# Langfuse (Self-Hosted Observability)
# Open-source LLM tracing - 100% data residency
# ===========================================
# 1. Start Langfuse: docker-compose -f docker-compose.local.yml up langfuse-server langfuse-db -d
# 2. Open http://localhost:3001 and create an account
# 3. Go to Settings → API Keys → Create new keys
# 4. Copy the keys below

# ===========================================
# LangGraph Server Configuration
# ===========================================
# By default, `langgraph dev` uses in-memory storage (data lost on restart)
# Set LANGGRAPH_POSTGRES_URI to enable PostgreSQL persistence for:
# - Checkpoints (agent state history)
# - Thread data (conversation persistence)
# - State snapshots (for replay/debugging)

# Local development (Docker)
LANGGRAPH_POSTGRES_URI=postgresql://yonca:yonca_dev_password@localhost:5433/yonca # pragma: allowlist secret

# Production (use your managed PostgreSQL)
# LANGGRAPH_POSTGRES_URI=postgresql://user:password@host:5432/yonca # pragma: allowlist secret

# ===========================================
# Langfuse Configuration
# ===========================================
YONCA_LANGFUSE_ENABLED=true
# Server-side authentication - used by your Python code to send traces
YONCA_LANGFUSE_SECRET_KEY=sk-lf-your-secret-key # pragma: allowlist secret
# Project identifier - identifies which project the traces belong to
YONCA_LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key # pragma: allowlist secret
YONCA_LANGFUSE_HOST=http://localhost:3001
YONCA_LANGFUSE_DEBUG=false
YONCA_LANGFUSE_SAMPLE_RATE=1.0  # 1.0 = trace 100%, 0.5 = trace 50%


# ===========================================
# Demo UI Authentication (Optional)
# ===========================================
# Google OAuth for tracking real users in Langfuse
# This is SEPARATE from synthetic farmer profiles!
#
# Setup:
# 1. Go to https://console.developers.google.com/apis/credentials
# 2. Create OAuth 2.0 Client ID (Web application)
# 3. Add redirect URI: http://localhost:8501/auth/oauth/google/callback
# 4. Copy Client ID and Secret below
#
# Benefits:
# - Your email shows in Langfuse traces (filter by user)
# - Track which developer ran which test
# - Still use synthetic farmer profiles for testing

OAUTH_GOOGLE_CLIENT_ID=your-client-id.apps.googleusercontent.com
OAUTH_GOOGLE_CLIENT_SECRET=your-client-secret
CHAINLIT_AUTH_SECRET=change-this-to-a-random-string

# Force account selection on every login (prevents auto-login to last account)
# Options: none, login, consent, select_account
OAUTH_GOOGLE_PROMPT=select_account

# ===========================================
# App Metadata
# ===========================================
YONCA_APP_NAME=ALEM
YONCA_APP_VERSION=0.1.0

# ===========================================
# Localization
# ===========================================
YONCA_DEFAULT_LANGUAGE=az

# ===========================================
# LangGraph Studio Configuration (Optional)
# ===========================================
# LangGraph Studio provides a visual debugger for agent workflows
# By default, `langgraph dev` uses in-memory storage (data lost on restart)
# Set LANGGRAPH_POSTGRES_URI to enable PostgreSQL persistence for:
# - Checkpoints (agent state history)
# - Thread data (conversation persistence)
# - State snapshots (for replay/debugging)

# Local development (Docker)
LANGGRAPH_POSTGRES_URI=postgresql://yonca:yonca_dev_password@localhost:5433/yonca # pragma: allowlist secret

# Production (use your managed PostgreSQL)
# LANGGRAPH_POSTGRES_URI=postgresql://user:password@host:5432/yonca # pragma: allowlist secret
