# ğŸŒ¿ Yonca AI - Headless Intelligence as a Service

> **Sidecar Intelligence Engine** for the Yonca agricultural platform.
> High-security, edge-ready AI backend with REST/GraphQL APIs, PII protection, and Azerbaijani language support.

## ğŸ¯ Overview

Yonca AI is a **Headless Intelligence as a Service** backendâ€”a detached, high-performance AI module that integrates seamlessly with existing platforms via API. Built with a "Logic-First" methodology, it delivers deterministic, rule-validated farm recommendations using 100% synthetic data.

**Key Architecture Principles:**
- **Sidecar Model**: Standalone AI engine that never touches core platform systems
- **Data Sovereignty**: 100% synthetic datasetsâ€”zero legal/operational friction
- **Edge-Ready**: Lightweight inference with Qwen2.5-7B for low-connectivity zones
- **Logic-First**: Deterministic agronomy rulebook overrides LLM hallucinations

## âœ¨ Features

- **PII Gateway** - Zero-trust data sanitization layer
- **RAG Engine** - Retrieval-Augmented Generation with agronomy rulebook
- **Lite Inference** - Edge-optimized GGUF quantization support
- **Rules Registry** - Deterministic agronomy rules with AZ- prefixes
- **Multi-LLM Support** - Google Gemini (cloud) or Qwen2.5 via Ollama (local)
- **Azerbaijani Language** - Native language support with Turkic dialect normalization
- **Trust Scores** - Confidence scoring with source citations
- **Digital Twin** - Simulation engine for scenario planning
- **REST & GraphQL APIs** - Flexible headless integration options
- **100% Synthetic Data** - Complete data safety, no real farmer data

## ğŸ—ï¸ Architecture

```
yonca-ai/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ yonca/
â”‚       â”œâ”€â”€ sidecar/          # ğŸ¯ CORE: Sidecar Intelligence Engine
â”‚       â”‚   â”œâ”€â”€ pii_gateway   # Zero-trust data sanitization
â”‚       â”‚   â”œâ”€â”€ rag_engine    # Retrieval-augmented generation
â”‚       â”‚   â”œâ”€â”€ rules_registry# Deterministic agronomy rules
â”‚       â”‚   â”œâ”€â”€ intent_matcher# Azerbaijani intent detection
â”‚       â”‚   â”œâ”€â”€ lite_inference# Edge-ready LLM inference
â”‚       â”‚   â”œâ”€â”€ trust         # Confidence scoring
â”‚       â”‚   â””â”€â”€ digital_twin  # Farm simulation
â”‚       â”œâ”€â”€ api/              # REST & GraphQL endpoints
â”‚       â”œâ”€â”€ agent/            # LangGraph AI orchestration
â”‚       â”œâ”€â”€ data/             # Synthetic scenarios & generators
â”‚       â”œâ”€â”€ models/           # Pydantic data models
â”‚       â””â”€â”€ startup.py        # Startup with Ollama health checks
â”œâ”€â”€ tests/                    # Test suite
â””â”€â”€ docs/                     # Documentation & API specs
```

## ğŸš€ Quick Start

### Prerequisites

#### Option A: Local LLM with Ollama (Recommended for Azerbaijani ğŸ‡¦ğŸ‡¿)

**Ollama is required for running local AI models.** This gives you:
- âœ… 100% offline capability
- âœ… No API costs
- âœ… Data never leaves your machine
- âœ… Best Azerbaijani language support with Qwen2.5

**Install Ollama:**

```bash
# Windows (via winget)
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh
```

> âš ï¸ **After installing Ollama, restart your terminal** for the PATH to update.

The Yonca startup manager will **automatically download the model** if it's not present!

#### Option B: Google Gemini (Cloud)

Get a free API key from [Google AI Studio](https://aistudio.google.com/apikey) and set it in `.env`:

```bash
GOOGLE_API_KEY=your-api-key-here
YONCA_LLM_PROVIDER=gemini
YONCA_LLM_MODEL=gemini-2.0-flash
```

### Installation

```bash
# Clone the repository
git clone https://github.com/ZekaLab/yonja.git
cd yonja

# Create virtual environment (Python 3.12)
python -m venv .venv312
.venv312\Scripts\activate  # Windows
source .venv312/bin/activate  # Linux/Mac

# Install with your preferred LLM provider
pip install -e ".[ollama]"    # For local Qwen2.5
pip install -e ".[gemini]"    # For Google Gemini
pip install -e ".[all-llms]"  # Both options
pip install -e ".[dev]"       # Development tools
```

### ğŸ® Start Yonca AI

**Option 1: VS Code (Recommended)**

Press `Ctrl+Shift+B` or run the task:
- **ğŸŒ¿ Start Yonca AI** - Full startup with Ollama health checks

**Option 2: Command Line**

```bash
# Automatic startup with health checks
python -m yonca.startup

# Or use the CLI command (after pip install -e .)
yonca
```

**Option 3: Check Status Only**

```bash
python -m yonca.startup --check-only
```

### What the Startup Manager Does

```
ğŸŒ¿ YONCA AI - Smart Farm Planning Assistant
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Ollama installed
âœ… Ollama server running
âœ… Model qwen2.5:7b ready

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ     ğŸŒ¿ Yonca AI Status             â”ƒ
â”£â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«
â”ƒ Component  â”ƒ Status                â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Ollama     â”‚ âœ… Running            â”‚
â”‚ LLM Model  â”‚ âœ… qwen2.5:7b         â”‚
â”‚ API        â”‚ ğŸš€ Starting...        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Starting Yonca AI API server...
INFO:     Uvicorn running on http://127.0.0.1:8000
```

### Access the API

- **REST API Docs**: http://localhost:8000/docs
- **GraphQL Playground**: http://localhost:8000/graphql

## ğŸ¤– LLM Configuration

| Provider | Model | Size | Best For |
|----------|-------|------|----------|
| **Ollama** | `qwen2.5:7b` | 4.7GB | ğŸ‡¦ğŸ‡¿ Azerbaijani (Recommended) |
| **Ollama** | `qwen2.5:3b` | 2.0GB | Fast responses, limited RAM |
| **Ollama** | `qwen2.5:14b` | 9.0GB | Highest quality |
| **Gemini** | `gemini-2.0-flash` | Cloud | Production, high volume |
| **Gemini** | `gemini-1.5-pro` | Cloud | Complex reasoning |

### Usage Example

```python
from yonca.agent import create_ollama_agent, create_gemini_agent

# Local Ollama (Azerbaijani-optimized)
agent = create_ollama_agent(model="qwen2.5:7b")
response = agent.chat("BuÄŸda sahÉ™sini nÉ™ vaxt suvarmaq lazÄ±mdÄ±r?")

# Google Gemini (Cloud)
agent = create_gemini_agent(api_key="your-key", model="gemini-2.0-flash")
response = agent.chat("TorpaÄŸÄ±n pH sÉ™viyyÉ™si nÉ™ olmalÄ±dÄ±r?")
```

## ğŸ“¡ API Endpoints

### REST API
```
POST /api/v1/recommendations     # Get AI recommendations
GET  /api/v1/farm/{id}/schedule  # Get daily schedule
POST /api/v1/chatbot/message     # Chat with assistant
GET  /api/v1/alerts/today        # Get today's alerts
```

### GraphQL
```graphql
query {
  farmRecommendations(farmId: "farm-001") {
    tasks { title priority dueDate }
    alerts { type severity message }
  }
}
```

## ğŸ§ª Testing

```bash
pytest tests/ -v --cov=src/yonca
```

## ğŸ“„ License

MIT License - ZekaLab Â© 2026

## ğŸ¤ Contributing

This is a prototype demonstration. For integration with the Yonca platform, contact ZekaLab.
