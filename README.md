# ðŸŒ¿ Yonca AI - Farm Planning Assistant

> **AI-powered daily farm recommendations for Azerbaijani farmers.**
> 100% offline-capable. 100% synthetic data. 100% rule-validated.

## ðŸŽ¯ What This Is

**Yonca AI** is a **Headless AI Sidecar** that generates personalized farming task lists by combining:
- **Local LLM** (Qwen3-4B via Ollama) for natural language in Azerbaijani
- **Deterministic Agronomy Rules** to ensure â‰¥90% logical accuracy
- **Synthetic Farm Scenarios** so no real farmer data is ever needed

It plugs into Digital Umbrella's Yonca platform without touching their existing EKTIS/subsidy systems.

## âœ¨ Core Features

| Feature | Purpose |
|---------|---------|
| **Rules Registry** | 20+ agronomy rules with AZ- prefixes (irrigation, fertilization, pest control) |
| **Intent Matcher** | Understands Azerbaijani farming questions |
| **Schedule Service** | Generates daily task lists with priorities |
| **Lite Inference** | 3 modes: `standard` (Ollama), `lite` (GGUF), `offline` (rules-only) |
| **PII Gateway** | Strips personal data before AI processing |
| **Trust Scores** | Every recommendation cites its source rule |

## ðŸ—ï¸ Architecture

```
yonca-ai/
â”œâ”€â”€ src/yonca/
â”‚   â”œâ”€â”€ sidecar/          # ðŸŽ¯ CORE: Headless Intelligence Engine
â”‚   â”‚   â”œâ”€â”€ rules_registry    # Single truth: agronomy rules
â”‚   â”‚   â”œâ”€â”€ intent_matcher    # Azerbaijani NLU
â”‚   â”‚   â”œâ”€â”€ schedule_service  # Daily task generation
â”‚   â”‚   â”œâ”€â”€ recommendation_service  # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ lite_inference    # Edge/offline modes
â”‚   â”‚   â”œâ”€â”€ pii_gateway       # Data sanitization
â”‚   â”‚   â””â”€â”€ trust             # Confidence scoring
â”‚   â”œâ”€â”€ api/              # REST & GraphQL endpoints
â”‚   â”œâ”€â”€ data/             # Synthetic scenarios (7 farm types)
â”‚   â”œâ”€â”€ models/           # Pydantic data models
â”‚   â””â”€â”€ umbrella/         # Streamlit demo UI
â”œâ”€â”€ tests/                # Test suite
â””â”€â”€ docs/                 # Documentation
```

**Key Principle:** The `sidecar/` is the intelligence engine. Everything else (API, UI) consumes it.

## ðŸš€ Quick Start

### Prerequisites

#### Option A: Local LLM with Ollama (Recommended for Azerbaijani ðŸ‡¦ðŸ‡¿)

**Ollama is required for running local AI models.** This gives you:
- âœ… 100% offline capability
- âœ… No API costs
- âœ… Data never leaves your machine
- âœ… Best Azerbaijani language support with Qwen3

**Install Ollama:**

```bash
# Windows (via winget)
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh
```

> âš ï¸ **After installing Ollama, restart your terminal** for the PATH to update.

The Yonca startup manager will **automatically download the model** if it's not present!

### Installation

> **ðŸŽ¯ Quick Tip:** Run `.\activate.ps1` or `activate.bat` for instant setup! See [COMMANDS.md](COMMANDS.md) for all usage options.

> **Tooling Note:** We use **Poetry** for dependency management (reads `pyproject.toml`, creates reproducible environments). **Uvicorn** is the ASGI server that runs FastAPIâ€”it's installed as a dependency, not a separate tool.

```bash
# Clone the repository
git clone https://github.com/ZekaLab/yonja.git
cd yonja

# Option A: Poetry (Recommended)
poetry install              # Core dependencies
poetry shell                # Activates the environment

# Option B: Quick activate script
.\activate.ps1              # Windows PowerShell
activate.bat                # Windows CMD

# Option C: pip + venv
python -m venv .venv
.venv\Scripts\activate      # Windows
source .venv/bin/activate   # Linux/Mac
pip install -e ".[dev]"     # Install in editable mode
```

**ðŸ’¡ Can't run `uvicorn` or `alembic` directly?**
â†’ See [COMMANDS.md](COMMANDS.md) for three ways to run commands without path issues.

### ðŸŽ® Start Yonca AI

**Option 1: VS Code (Recommended)**

Press `Ctrl+Shift+B` or run the task:
- **ðŸŒ¿ Start Yonca AI** - Full startup with Ollama health checks

**Option 2: Command Line**

```bash
# Automatic startup with health checks
python -m yonca.startup

# Or use the CLI command (after pip install -e .)
yonca
```

**Option 3: Check Status Only**

```bash
python -m yonca.startup --check-only
```

### What the Startup Manager Does

```
ðŸŒ¿ YONCA AI - Smart Farm Planning Assistant
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Ollama installed
âœ… Ollama server running
âœ… Model qwen3:4b ready

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ     ðŸŒ¿ Yonca AI Status             â”ƒ
â”£â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«
â”ƒ Component  â”ƒ Status                â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Ollama     â”‚ âœ… Running            â”‚
â”‚ LLM Model  â”‚ âœ… qwen3:4b           â”‚
â”‚ API        â”‚ ðŸš€ Starting...        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Starting Yonca AI API server...
INFO:     Uvicorn running on http://127.0.0.1:8000
```

### Access the API

- **REST API Docs**: http://localhost:8000/docs
- **GraphQL Playground**: http://localhost:8000/graphql

## ðŸ¤– LLM Configuration

### Two Deployment Modes

**MODE 1: Groq Cloud (Benchmark)**
- Purpose: Proves what's possible with open-source models
- Performance: 200-300 tok/s (enterprise-grade)
- Cost: $0-50/mo (free tier available)
- Use for: Development, testing, proof-of-concept

**MODE 2: DigiRella Self-Hosted (Production)**
- Purpose: Same performance as Groq, your infrastructure
- Performance: 200-300 tok/s (matches Groq)
- Cost: $2,600-145k one-time OR $470-7,600/mo (rented GPU)
- Use for: Production, data sovereignty, air-gapped

ðŸ“š See [PRICING-SIMPLIFIED.md](docs/PRICING-SIMPLIFIED.md) for full comparison

### Available Models (All Open-Source)

| Provider | Model | Deployment | Performance |
|----------|-------|------------|-------------|
| **Groq** | `llama-4-maverick-17b` | Cloud | ðŸš€ 300 tok/s (benchmark) |
| **Groq** | `llama-3.3-70b` | Cloud | ðŸš€ 200+ tok/s |
| **Groq** | `qwen3-32b` | Cloud | ðŸš€ 280 tok/s |
| **DigiRella** | Same models | Self-hosted | ðŸ  200-300 tok/s |
| **Ollama** | `qwen3:4b` | Local | ðŸ‡¦ðŸ‡¿ Offline-capable |
| **Ollama** | `atllama:7b` | Local | ðŸ‡¦ðŸ‡¿ Azerbaijani-tuned |

> **Key:** Groq = Cloud benchmark | DigiRella = Self-hosted equivalent | Ollama = Lightweight local

### Setting Up Local Models (Ollama)

**Option 1: Docker (Recommended)**
```bash
# Start all services and setup models
docker-compose -f docker-compose.local.yml up -d

# First-time setup: pull qwen3 and import ATLLaMA
docker-compose -f docker-compose.local.yml --profile setup up model-setup
```

**Option 2: Manual Setup**
```powershell
# Pull Qwen3 (primary model)
ollama pull qwen3:4b

# Import ATLLaMA from GGUF (Azerbaijani-tuned)
python scripts/import_model.py --name atllama --path models/atllama.v3.5.Q4_K_M.gguf

# Or import into Docker container
python scripts/import_model.py --docker
```

### Switching Models

Set the model via environment variable:
```bash
YONCA_OLLAMA_MODEL=qwen3:4b   # Qwen3 (default)
YONCA_OLLAMA_MODEL=atllama    # ATLLaMA (Azerbaijani)
```

Or use the API:
```bash
# List available models
curl http://localhost:8000/api/models

# Check model status
curl http://localhost:8000/api/models/qwen3:4b
```

### Usage Example

```python
from yonca.agent import create_ollama_agent
from yonca.llm import create_groq_provider

# Local Ollama (Azerbaijani-optimized)
agent = create_ollama_agent(model="qwen3:4b")
response = agent.chat("BuÄŸda sahÉ™sini nÉ™ vaxt suvarmaq lazÄ±mdÄ±r?")

# Groq Cloud (Ultra-fast open-source models - benchmark)
llm = create_groq_provider(api_key="your-key", model="llama-4-maverick-17b-128e-instruct")
response = await llm.generate([
    LLMMessage.user("TorpaÄŸÄ±n pH sÉ™viyyÉ™si nÉ™ olmalÄ±dÄ±r?")
])
```

## ðŸš€ Deployment Options

Yonca AI supports three deployment tiers with the same open-source models:

| Tier | Infrastructure | Cost | Data Location | Performance |
|------|----------------|------|---------------|-------------|
| **Groq Cloud** | Cloud API (benchmark) | $0-50/mo | US | 200-300 tok/s |
| **DigiRella Cloud** | Rented GPU (AzInTelecom) | $800-1,500/mo | ðŸ‡¦ðŸ‡¿ Azerbaijan | 200-300 tok/s |
| **DigiRella Owned** | Self-hosted hardware | $2,600-145k one-time | Your premises | 200-300 tok/s |

**Key Principle:** Groq demonstrates the benchmark. DigiRella provides the path to replicate that performance with data sovereignty.

ðŸ“š **Full Details:**
- [PRICING-SIMPLIFIED.md](docs/PRICING-SIMPLIFIED.md) â€” Cost comparison & migration path
- [17-DIGIRELLA-HOSTING-PROFILES.md](docs/zekalab/17-DIGIRELLA-HOSTING-PROFILES.md) â€” Hardware specs
- [16-ALEM-INFRASTRUCTURE-TIERS.md](docs/zekalab/16-ALEM-INFRASTRUCTURE-TIERS.md) â€” Tier comparison

## ðŸ“¡ API Endpoints

### REST API (`/api/v1/`)
```
GET  /farms                 â†’ List 7 synthetic farm scenarios
GET  /farms/{id}            â†’ Get specific farm profile
POST /recommendations       â†’ Get AI recommendations
GET  /farms/{id}/schedule   â†’ Get daily task schedule
POST /chatbot/message       â†’ Chat in Azerbaijani
GET  /alerts/today          â†’ Get weather/disease alerts
```

### Sidecar API (`/api/v1/sidecar/`)
```
POST /recommendations       â†’ Full pipeline with PII gateway
GET  /status                â†’ Service health + inference mode
POST /mode/{mode}           â†’ Switch: standard/lite/offline
GET  /rulebook              â†’ View agronomy rules (AZ- prefixes)
```

---

## ðŸ“Š Success Metrics

| Metric | Target | How We Achieve It |
|--------|--------|-------------------|
| **Logical Accuracy** | â‰¥90% | Rules Registry validates every LLM output |
| **Data Safety** | 100% | PII Gateway + Synthetic data only |
| **Offline Capability** | Yes | `offline` mode uses rules-only, no network |
| **Azerbaijani Support** | Native | Intent Matcher with Turkic dialect handling |
| **Integration Ready** | Yes | Same API contract as Yonca platform |

---

## ðŸ§ª Testing

```bash
pytest tests/ -v --tb=short
```

### Testing the API

```bash
# Check chat endpoint info
curl http://localhost:8000/api/v1/chat

# Send a message
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Salam! BuÄŸda É™kini haqqÄ±nda mÉ™lumat verÉ™ bilÉ™rsinizmi?",
    "user_id": "farmer_123",
    "stream": false
  }'

# Stream responses
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Pomidor É™kini Ã¼Ã§Ã¼n É™n yaxÅŸÄ± vaxt nÉ™ vaxtdÄ±r?",
    "stream": true
  }'
```

## ðŸ“„ License

MIT License - ZekaLab Â© 2026
