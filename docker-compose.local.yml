# Docker Compose for Local Development
# Usage: docker-compose -f docker-compose.local.yml up -d

services:
  # ================================================
  # Yonca AI API (Development Mode)
  # ================================================
  api:
    build:
      context: .
      target: development
      dockerfile: Dockerfile
    container_name: yonca-api
    ports:
      - "8000:8000"
    volumes:
      # Mount source code for hot reload
      - ./src:/app/src
      - ./prompts:/app/prompts
      - ./data:/app/data
    environment:
      # Deployment settings
      - DEPLOYMENT_MODE=local
      - ENVIRONMENT=development
      - DEBUG=true

      # API settings
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - CORS_ORIGINS=http://localhost:3000,http://localhost:8501

      # LLM Provider (use Ollama for local)
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=qwen3:4b

      # Database (PostgreSQL for production parity)
      - DATABASE_URL=postgresql+asyncpg://yonca:yonca_dev_password@postgres:5432/yonca

      # Redis
      - REDIS_URL=redis://redis:6379/0

      # Security (development only - change in production!)
      - JWT_SECRET=dev-secret-change-in-production
      - JWT_ALGORITHM=HS256
      - JWT_EXPIRY_HOURS=24

      # Observability
      - LOG_LEVEL=DEBUG
      - LOG_FORMAT=console
      - PROMETHEUS_ENABLED=true

      # Rate limiting
      - RATE_LIMIT_REQUESTS_PER_MINUTE=100
      - RATE_LIMIT_BURST=150
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - yonca-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ================================================
  # Ollama (Local LLM Server)
  # ================================================
  ollama:
    image: ollama/ollama:latest
    container_name: yonca-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      # Mount local models directory for GGUF imports
      - ./models:/app/models:ro
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - yonca-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    # Uncomment for GPU support (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # ================================================
  # Model Setup (One-time initialization)
  # Pulls qwen3:4b and imports GGUF models
  # ================================================
  model-setup:
    image: ollama/ollama:latest
    container_name: yonca-model-setup
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./models:/app/models:ro
    environment:
      - OLLAMA_HOST=ollama:11434
    networks:
      - yonca-network
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "ðŸš€ Setting up models..."

        # Pull Qwen3 (primary model)
        echo "ðŸ“¥ Pulling qwen3:4b..."
        ollama pull qwen3:4b

        # Import ATLLaMA from GGUF if file exists
        if [ -f /app/models/atllama.v3.5.Q4_K_M.gguf ]; then
          echo "ðŸ“¦ Importing ATLLaMA from GGUF..."
          cat > /tmp/Modelfile << 'EOF'
        FROM /app/models/atllama.v3.5.Q4_K_M.gguf
        SYSTEM "SÉ™n AzÉ™rbaycan fermerlÉ™rÉ™ kÃ¶mÉ™k edÉ™n sÃ¼ni intellekt kÃ¶mÉ™kÃ§isisÉ™n. SuallarÄ± AzÉ™rbaycan dilindÉ™ cavablandÄ±r."
        PARAMETER temperature 0.7
        PARAMETER num_ctx 4096
        EOF
          ollama create atllama -f /tmp/Modelfile
          echo "âœ… ATLLaMA imported!"
        else
          echo "âš ï¸ ATLLaMA GGUF not found, skipping..."
        fi

        echo "ðŸ“‹ Available models:"
        ollama list
        echo "âœ… Model setup complete!"
    profiles:
      - setup  # Only runs with: docker-compose --profile setup up model-setup

  # ================================================
  # LangGraph Dev Server (Production-like API)
  # ================================================
  langgraph:
    build:
      context: .
      dockerfile: Dockerfile.langgraph
    container_name: yonca-langgraph
    ports:
      - "2024:2024"
    volumes:
      # Mount source code for hot reload
      - ./src:/app/src
      - ./langgraph.json:/app/langgraph.json
    environment:
      # Python path (CRITICAL for imports)
      - PYTHONPATH=/app/src

      # LangGraph server settings
      - LANGGRAPH_DEV_HOST=0.0.0.0
      - LANGGRAPH_DEV_PORT=2024
      - LANGGRAPH_API_LOG_LEVEL=INFO

      # LangGraph PostgreSQL persistence (enables checkpoint/state storage)
      # Without this, LangGraph uses in-memory storage (data lost on restart)
      - LANGGRAPH_POSTGRES_URI=postgresql://yonca:yonca_dev_password@postgres:5432/yonca

      # LLM Provider
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=qwen3:4b

      # Database (for app data, separate from LangGraph checkpoints)
      - DATABASE_URL=postgresql+asyncpg://yonca:yonca_dev_password@postgres:5432/yonca

      # Redis for caching (optional, LangGraph uses Postgres for checkpoints)
      - REDIS_URL=redis://redis:6379/0

      # Langfuse observability
      - LANGFUSE_ENABLED=true
      - LANGFUSE_SECRET_KEY=${YONCA_LANGFUSE_SECRET_KEY:-}
      - LANGFUSE_PUBLIC_KEY=${YONCA_LANGFUSE_PUBLIC_KEY:-}
      - LANGFUSE_HOST=http://langfuse-server:3000
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - yonca-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:2024/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    profiles:
      - langgraph  # Only runs with: docker-compose --profile langgraph up

  # ================================================
  # Redis Stack (Caching, Session Storage & LangGraph Checkpointing)
  # Includes: RedisJSON, RediSearch (required for langgraph-checkpoint-redis)
  # ================================================
  redis:
    image: redis/redis-stack-server:latest
    container_name: yonca-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - yonca-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ================================================
  # Langfuse (Self-Hosted Observability)
  # Open-source LLM observability - No license fees!
  # Data residency: All traces stay within your infrastructure
  # ================================================
  langfuse-server:
    image: langfuse/langfuse:2
    container_name: yonca-langfuse
    ports:
      - "3001:3000"  # Langfuse UI at http://localhost:3001
    environment:
      # Database connection
      - DATABASE_URL=postgresql://langfuse:langfuse_secret@langfuse-db:5432/langfuse
      - DIRECT_URL=postgresql://langfuse:langfuse_secret@langfuse-db:5432/langfuse

      # Security (CHANGE IN PRODUCTION!)
      - NEXTAUTH_SECRET=yonca-langfuse-secret-change-in-prod
      - SALT=yonca-langfuse-salt-change-in-prod
      - NEXTAUTH_URL=http://localhost:3001

      # Telemetry (disable for air-gapped)
      - TELEMETRY_ENABLED=false

      # Optional: Enable public sign-up for first user
      - AUTH_DISABLE_SIGNUP=false

      # ClickHouse for high-volume tracing (optional)
      # - CLICKHOUSE_URL=http://langfuse-clickhouse:8123
    depends_on:
      langfuse-db:
        condition: service_healthy
    networks:
      - yonca-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/public/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ================================================
  # Langfuse PostgreSQL Database
  # Stores all AI traces, prompts, and evaluations
  # ================================================
  langfuse-db:
    image: postgres:15-alpine
    container_name: yonca-langfuse-db
    volumes:
      - langfuse-db-data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=langfuse
      - POSTGRES_USER=langfuse
      - POSTGRES_PASSWORD=langfuse_secret
    networks:
      - yonca-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U langfuse -d langfuse"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ================================================
  # PostgreSQL (Yonca Application Database)
  # ================================================
  postgres:
    image: postgres:15-alpine
    container_name: yonca-postgres
    ports:
      - "5433:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=yonca
      - POSTGRES_USER=yonca
      - POSTGRES_PASSWORD=yonca_dev_password
    networks:
      - yonca-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U yonca"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ================================================
  # Chainlit Demo UI
  # ================================================
  demo-ui:
    build:
      context: .
      dockerfile: demo-ui/Dockerfile
      target: development
    container_name: yonca-demo-ui
    ports:
      - "8501:8501"
    volumes:
      # Mount source code for hot reload
      - ./src:/app/src
      - ./demo-ui:/app/demo-ui
      - ./prompts:/app/prompts
      - ./data:/app/data
    environment:
      # LLM settings
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=qwen3:4b

      # Redis for checkpointing
      - REDIS_URL=redis://redis:6379/0

      # Database (PostgreSQL for data persistence)
      - DATABASE_URL=postgresql+asyncpg://yonca:yonca_dev_password@postgres:5432/yonca
      - CHAINLIT_DATABASE_URL=postgresql+asyncpg://yonca:yonca_dev_password@postgres:5432/yonca
      - ENABLE_DATA_PERSISTENCE=true

      # Yonca API (optional, for API client pattern)
      - YONCA_API_URL=http://api:8000

      # Python path
      - PYTHONPATH=/app/src:/app/demo-ui
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - yonca-network
    restart: unless-stopped
    profiles:
      - demo  # Only runs with: docker-compose --profile demo up

# ================================================
# Networks
# ================================================
networks:
  yonca-network:
    driver: bridge

# ================================================
# Volumes
# ================================================
volumes:
  ollama-data:
    driver: local
  redis-data:
    driver: local
  langfuse-db-data:
    driver: local
  postgres-data:
    driver: local
