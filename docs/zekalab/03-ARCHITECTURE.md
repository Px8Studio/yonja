# âš™ï¸ Yonca AI â€” Technical Architecture

> **Purpose:** Complete technical reference for the Sidecar Intelligence Moduleâ€”components, APIs, deployment, and roadmap.

---

## ğŸ§  Core Technology: LangGraph Agentic Framework

We propose building the Yonca AI Sidecar using **LangGraph**â€”an enterprise-grade agentic framework that transforms the system from a simple "input-output" advisor into a **Stateful Farming Orchestrator** that reasons, remembers, and self-corrects.

### Why LangGraph?

| Capability | Benefit for Digital Umbrella |
|:-----------|:-----------------------------|
| **Graph-Based Logic** | Visual flowchart of AI decision-makingâ€”auditable by non-technical agronomists |
| **Native Checkpointing** | Farmer loses connection mid-chat? LangGraph saves the exact conversation state |
| **Vendor Agnostic** | Deployable on any cloud (Azure, AWS) or local Baku serversâ€”meets Data Safety requirements |
| **Human-in-the-Loop** | Built-in interrupt nodes for verifying risky agricultural advice before delivery |
| **Cycles & Loops** | Validation loops catch incorrect recommendations *before* the farmer sees them |

### Agentic Architecture: The Supervisor Pattern

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph TB
    subgraph supervisor["ğŸ¯ Supervisor Agent"]
        sup["Orchestrator<br/><i>Routes tasks to specialists</i>"]
    end
    
    subgraph specialists["ğŸ‘¥ Specialist Sub-Agents"]
        agro["ğŸŒ¾ Agronomist Agent<br/><i>Sowing dates, crop cycles</i>"]
        subsidy["ğŸ’° Subsidy Specialist<br/><i>EKTIS deadlines, applications</i>"]
        weather["â›… Weather Analyst<br/><i>Forecast monitoring</i>"]
    end
    
    subgraph memory["ğŸ’¾ Memory Layer"]
        short["Short-Term<br/><i>Current session</i>"]
        long["Long-Term<br/><i>Farmer history via Checkpointer</i>"]
    end
    
    sup --> agro
    sup --> subsidy
    sup --> weather
    agro --> short
    subsidy --> short
    weather --> short
    short <--> long
    
    style supervisor fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px,color:#1b5e20
    style specialists fill:#e3f2fd,stroke:#1565c0,color:#0d47a1
    style memory fill:#fff9c4,stroke:#f9a825,color:#5d4037
```

The system **remembers context**â€”if a farmer mentioned a pest issue three days ago, the assistant recalls it in subsequent sessions, even when using synthetic profiles.

---

## ğŸ¯ Why "Sidecar"?

Digital Umbrella's Yonca platform handles **legal government data** (subsidy applications, EKTIS integration). We can't access that, so we run as a **sidecar**â€”a separate AI module.

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph LR
    subgraph protected["ğŸ”’ Protected Zone"]
        yonca["ğŸ›ï¸ YONCA PLATFORM<br/><i>Legal/Financial Data</i>"]
    end
    
    subgraph sandbox["ğŸ§ª Sandbox Zone"]
        sidecar["ğŸŒ¿ AI SIDECAR<br/><i>Recommendations Only</i>"]
    end
    
    yonca <-.->|"API Contract<br/>(No DB Access)"| sidecar
    
    style protected fill:#ffcdd2,stroke:#c62828,stroke-width:3px,color:#b71c1c
    style sandbox fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#1b5e20
```

### Four Guarantees

| # | Guarantee | Implementation |
|:-:|:----------|:---------------|
| 1 | **Never touches database** | Uses synthetic farm scenarios only |
| 2 | **Strips all PII** | Farmer names/IDs hashed before AI processing |
| 3 | **Validates with rules** | Every LLM output checked against agronomy rulebook |
| 4 | **Ready to plug in** | Same API contract, just flip data source later |

---

## ğŸŒ Ecosystem Context

### The Current Landscape

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph TB
    subgraph farmer["ğŸ§‘â€ğŸŒ¾ Farmer"]
        phone["ğŸ“± Smartphone<br/><i>Low-bandwidth</i>"]
    end
    
    subgraph platform["ğŸ›ï¸ Yonca Platform"]
        app["Yonca App"]
        ektis["EKTIS<br/><i>Gov System</i>"]
        satellite["ğŸ›°ï¸ Satellite<br/>Monitoring"]
    end
    
    subgraph gov["ğŸ‡¦ğŸ‡¿ Government"]
        subsidy["ğŸ’° Subsidy<br/>Processing"]
        registry["ğŸ“‹ Land<br/>Registry"]
    end
    
    phone --> app
    app <--> ektis
    app --> satellite
    ektis <--> subsidy
    ektis <--> registry
    
    style farmer fill:#e8f5e9,stroke:#2e7d32,color:#1b5e20
    style platform fill:#e1f5fe,stroke:#01579b,color:#01579b
    style gov fill:#fff9c4,stroke:#f9a825,color:#5d4037
```

| Aspect | Current State |
|:-------|:--------------|
| **Platform Role** | Primary digital gateway to EKTIS for Azerbaijani farmers |
| **User Persona** | Small-to-medium holders (~1.6 ha average), mobile-first |
| **Tech Maturity** | Modern stack with Data Engineering & Satellite Monitoring |
| **Critical Constraint** | Data Privacy â€” government-linked subsidy/land data |

### Technical Discovery Gaps

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph LR
    subgraph gaps["â“ Questions for Digital Umbrella"]
        q1["ğŸ“± Mobile Framework?<br/><i>Flutter/React Native/Native</i>"]
        q2["ğŸ“¶ Offline Handling?<br/><i>State persistence strategy</i>"]
        q3["ğŸ¨ Design System?<br/><i>Figma, hex codes, typography</i>"]
        q4["â˜ï¸ Hosting?<br/><i>Their infra vs standalone Docker</i>"]
    end
    
    style gaps fill:#fff3e0,stroke:#ef6c00,color:#e65100
```

---

## ï¿½ Integration Bridge: FastAPI + LangGraph Server

Since Yonca is a mobile app and LangGraph is a Python-based framework, we provide a robust API bridge ensuring smooth handoff between systems.

### Request Flow Architecture

```mermaid
sequenceDiagram
    participant M as ğŸ“± Yonca Mobile App
    participant F as ğŸ”Œ FastAPI Gateway
    participant L as ğŸ§  LangGraph Server
    participant S as ğŸ“Š Synthetic Data
    participant R as ğŸ“š Rulebook
    
    M->>F: POST /yonca-ai/chat
    F->>L: Start Thread (farmer_id)
    L->>L: Load Graph State
    L->>S: Fetch Weather/Soil Data
    S-->>L: Synthetic Scenario
    L->>R: Validate Against Rules
    R-->>L: Rule Matches
    L->>L: Execute Nodes
    L-->>F: SSE Stream Response
    F-->>M: Real-time "typing" effect
```

### Server-Sent Events (SSE) Streaming

The farmer sees the AI "typing" its reasoning in real-timeâ€”creating a premium, responsive experience:

```python
# Streaming endpoint example
@app.post("/yonca-ai/chat")
async def chat_endpoint(request: ChatRequest):
    thread_id = f"farmer_{request.farm_id}"
    
    async def generate():
        async for event in graph.astream(
            {"messages": request.messages},
            config={"configurable": {"thread_id": thread_id}}
        ):
            yield f"data: {json.dumps(event)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

### Single Endpoint Simplicity

Digital Umbrella's IT team doesn't need to understand LangGraph internalsâ€”they simply call:

```
POST /yonca-ai/chat
```

The entire intelligence module is a **Dockerized Microservice** ready to deploy.

---

## ï¿½ğŸ“ System Architecture

### Level 0: Context Diagram

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph TB
    subgraph platform["ğŸ›ï¸ YONCA PLATFORM"]
        restapi["EXISTING REST API<br/><code>/api/v1/farms</code><br/><code>/api/v1/recommendations</code><br/><code>/api/v1/chatbot</code>"]
        
        subgraph sidecar["ğŸ§  SIDECAR INTELLIGENCE MODULE"]
            api["ğŸ”Œ /sidecar/recommendations"]
            pii["ğŸ›¡ï¸ PII Gateway<br/><i>Sanitize</i>"]
            rag["ğŸ¤– RAG Engine<br/><i>Qwen2.5</i>"]
            lite["âš¡ Lite-Inference<br/><i>GGUF</i>"]
            rules["ğŸ“š Agronomy<br/>Rulebook"]
            
            api --> pii
            pii --> rag
            lite --> rag
            rag --> rules
        end
        
        subgraph synthetic["ğŸ§ª SYNTHETIC DATA LAYER"]
            weather["â˜ï¸ Weather<br/>Generator"]
            soil["ğŸœï¸ Soil<br/>Generator"]
            profiles["ğŸ§‘â€ğŸŒ¾ Farm<br/>Profiles"]
            scenarios["ğŸ“‹ Scenario<br/>Farms"]
        end
        
        restapi -->|"No DB Access"| api
        rag --> synthetic
    end
    
    subgraph future["ğŸ”® FUTURE: National Ecosystem"]
        asan["ASAN KÉ™nd API"]
        azerstat["AzerStat Data"]
        agribank["AgriBank Subsidy"]
        egov["e-Gov Identity"]
    end
    
    sidecar -.->|"Ready-to-Plug"| future
    
    style sidecar fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px,color:#1b5e20
    style synthetic fill:#e3f2fd,stroke:#1565c0,color:#0d47a1
    style rules fill:#fff9c4,stroke:#f9a825,stroke-width:2px,color:#5d4037
```

### Data Flow

```mermaid
sequenceDiagram
    participant F as ğŸ§‘â€ğŸŒ¾ Farmer
    participant A as ğŸ“± Yonca App
    participant P as ğŸ›¡ï¸ PII Gateway
    participant R as ğŸ¤– RAG Engine
    participant L as ğŸ“š Rulebook
    participant M as ğŸ§  LLM
    
    F->>A: Query (Azerbaijani)
    A->>P: Raw Request
    P->>P: Sanitize PII
    P->>R: Safe Request
    R->>L: Check Rules
    L-->>R: Matching Rules
    R->>M: Generate Response
    M-->>R: LLM Output
    R->>R: Validate (>90%)
    R-->>P: Verified Advice
    P->>P: Re-personalize
    P-->>A: Final Response
    A-->>F: Localized Advice
```

---

## ğŸ§© Architecture Components

### 1. PII-Stripping Gateway

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
flowchart LR
    subgraph ingest["â‘  INGEST"]
        raw["Raw Request"]
        sanitize["sanitize()"]
        safe["Safe Request"]
        raw --> sanitize --> safe
    end
    
    subgraph process["â‘¡ PROCESS"]
        rag["RAG Engine"]
        safeRes["Safe Response"]
        safe --> rag --> safeRes
    end
    
    subgraph egress["â‘¢ EGRESS"]
        personal["personalize()"]
        final["Final Response"]
        safeRes --> personal --> final
    end
    
    style ingest fill:#ffcdd2,stroke:#c62828,color:#b71c1c
    style process fill:#fff9c4,stroke:#f9a825,color:#5d4037
    style egress fill:#c8e6c9,stroke:#2e7d32,color:#1b5e20
```

**Location:** `src/yonca/sidecar/pii_gateway.py`

| Feature | Treatment |
|:--------|:----------|
| Azerbaijani name patterns | "Æli MÉ™mmÉ™dov oÄŸlu" â†’ `[ÅÆXS_1]` |
| Phone numbers (+994) | Stripped â†’ SHA-256 hash only |
| GPS coordinates | Anonymized â†’ Region code only |
| Farm/Farmer IDs | Tokenized â†’ `syn_abc123` |

---

### 2. LangGraph Orchestration Engine

**Location:** `src/yonca/sidecar/graph_engine.py`

The heart of the systemâ€”a **stateful graph** that orchestrates all AI decision-making with built-in safety and memory.

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
flowchart TB
    subgraph graph["ğŸ§  LangGraph State Machine"]
        state["ğŸ“‹ State<br/><i>TypedDict: farm_profile,<br/>weather, chat_history</i>"]
        
        subgraph nodes["Processing Nodes"]
            analyze["ğŸ” Scenario Analyzer<br/><i>Parse farmer context</i>"]
            recommend["ğŸ’¡ Recommendation Engine<br/><i>Generate advice</i>"]
            guard["ğŸ›¡ï¸ Safety Guardrail<br/><i>Validate output</i>"]
            redline["ğŸš« Redline Scanner<br/><i>Zero real data check</i>"]
        end
        
        subgraph edges["Conditional Routing"]
            cond1{"Risk Level?"}
            cond2{"Data Leak?"}
        end
        
        state --> analyze
        analyze --> recommend
        recommend --> cond1
        cond1 -->|"High"| guard
        cond1 -->|"Low/Medium"| cond2
        guard --> cond2
        cond2 -->|"Clean"| output["âœ… Response"]
        cond2 -->|"Detected"| redline
        redline -->|"Retry"| recommend
    end
    
    style nodes fill:#e3f2fd,stroke:#1565c0,color:#0d47a1
    style guard fill:#fff9c4,stroke:#f9a825,stroke-width:2px,color:#5d4037
    style redline fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#b71c1c
```

#### State Schema

```python
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver

class FarmingState(TypedDict):
    """Complete state for farming advisory session."""
    farm_profile: dict          # Synthetic farm data
    weather_data: list[dict]    # Recent/forecast weather
    soil_conditions: dict       # Current soil metrics
    chat_history: list[dict]    # Conversation memory
    current_query: str          # User's question
    recommendations: list[dict] # Generated advice
    risk_level: str             # low | medium | high
    confidence_score: float     # 0.0 - 1.0
    validation_notes: list[str] # Audit trail
```

#### Graph Definition

```python
# Build the LangGraph
graph = StateGraph(FarmingState)

# Add processing nodes
graph.add_node("scenario_analyzer", analyze_scenario)
graph.add_node("recommendation_engine", generate_recommendations)
graph.add_node("safety_guardrail", validate_safety)
graph.add_node("redline_scanner", scan_for_real_data)

# Add conditional edges
graph.add_conditional_edges(
    "recommendation_engine",
    route_by_risk,
    {"high": "safety_guardrail", "low": "redline_scanner", "medium": "redline_scanner"}
)

# Compile with persistence
checkpointer = MemorySaver()  # Or PostgresSaver for production
app = graph.compile(checkpointer=checkpointer)
```

#### The "Redline" Compliance Node

Digital Umbrella's top priority is **Data Safety**. The Redline Scanner automatically:

- Scans every AI response for real data patterns
- Blocks any hallucinated PII or actual farm IDs
- Ensures **zero real data** leakage
- Acts as an automated compliance officer

```python
def scan_for_real_data(state: FarmingState) -> FarmingState:
    """Compliance node: ensures no real data in responses."""
    response_text = state["recommendations"][-1]["description"]
    
    violations = []
    # Check for real phone patterns
    if re.search(r'\+994\d{9}', response_text):
        violations.append("Phone number detected")
    # Check for real farm ID patterns
    if re.search(r'FARM-\d{6}', response_text):
        violations.append("Real farm ID detected")
    
    if violations:
        state["validation_notes"].extend(violations)
        # Route back to recommendation engine for retry
        return {**state, "retry_required": True}
    
    return {**state, "retry_required": False}
```

---

### 3. RAG Engine with Rulebook

**Location:** `src/yonca/sidecar/rag_engine.py`

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
flowchart TB
    subgraph pipeline["ğŸ¤– RAG Pipeline"]
        step1["â‘  Intent Detection<br/><i>Azerbaijani â†’ category</i>"]
        step2["â‘¡ Knowledge Retrieval<br/><i>Semantic search</i>"]
        step3["â‘¢ Rule Evaluation<br/><i>Deterministic</i>"]
        step4["â‘£ LLM Generation<br/><i>Qwen2.5-7B</i>"]
        step5["â‘¤ Validation<br/><i>>90% accuracy</i>"]
        
        step1 --> step2 --> step3 --> step4 --> step5
    end
    
    style step3 fill:#fff9c4,stroke:#f9a825,stroke-width:2px,color:#5d4037
    style step5 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
```

**Rulebook Categories:**

| Category | # Rules | Purpose | Example Rule |
|:---------|:--------|:--------|:-------------|
| ğŸ’§ Irrigation | 4 | Water management | moisture < 30% â†’ irrigate |
| ğŸ§ª Fertilization | 3 | Nutrient application | N < 20 kg/ha â†’ add nitrogen |
| ğŸ› Pest Control | 2 | Disease prevention | humidity > 80% â†’ fungicide alert |
| ğŸŒ¾ Harvest | 2 | Optimal timing | maturity + dry weather = harvest |
| ğŸ„ Livestock | 2 | Animal care | temperature > 35Â°C â†’ shade/water |
| ğŸœï¸ Soil Management | 2 | pH/nutrient correction | pH < 6 â†’ lime application |

---

### 4. Lite-Inference Engine

**Location:** `src/yonca/sidecar/lite_inference.py`

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph LR
    subgraph modes["âš¡ Inference Modes"]
        standard["ğŸ–¥ï¸ STANDARD<br/><i>Full Qwen2.5-7B</i><br/><i>Ollama</i>"]
        lite["ğŸ“± LITE<br/><i>Quantized GGUF</i><br/><i><4.5GB RAM</i>"]
        offline["ğŸ“¶ OFFLINE<br/><i>Pure Rules</i><br/><i><50ms latency</i>"]
    end
    
    style standard fill:#bbdefb,stroke:#1565c0,color:#0d47a1
    style lite fill:#fff9c4,stroke:#f9a825,color:#5d4037
    style offline fill:#c8e6c9,stroke:#2e7d32,color:#1b5e20
```

**GGUF Model Options:**

| Model | Quantization | Memory | Speed | Use Case |
|:------|:-------------|:-------|:------|:---------|
| qwen2.5-7b | Q4_K_M | 4.5GB | 15 tok/s | Full capability |
| qwen2.5-7b | Q5_K_M | 5.5GB | 12 tok/s | Quality priority |
| qwen2.5-3b | Q4_K_M | 2.0GB | 25 tok/s | Mobile server |
| qwen2.5-1.5b | Q4_K_M | 1.2GB | 40 tok/s | Edge device |

---

## ğŸ›¤ï¸ Dummy-to-Real Roadmap

### Three-Phase Transition

```mermaid
timeline
    title Data Transition Roadmap
    
    section Phase 1: Prototype
        0-6 months : 100% Synthetic Data
                   : Scenario farms
                   : Generated weather
                   : LOW risk
    
    section Phase 2: Hybrid  
        6-12 months : Real + Synthetic Blend
                    : Regional statistics
                    : Anonymized farms
                    : MEDIUM risk
    
    section Phase 3: Production
        12-24 months : Real Data (PII Protected)
                     : ASAN KÉ™nd API
                     : Federated learning
                     : HIGH risk (managed)
```

### Phase Details

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
flowchart LR
    subgraph phase1["ğŸ“¦ Phase 1: Prototype<br/><i>Current</i>"]
        syn["100% Synthetic<br/>â€¢ Scenario farms<br/>â€¢ Generated weather<br/>â€¢ PII Gateway"]
    end
    
    subgraph phase2["ğŸ”€ Phase 2: Hybrid"]
        blend["Real + Synthetic<br/>â€¢ AzerStat regional<br/>â€¢ k-anonymity (kâ‰¥10)<br/>â€¢ IoT aggregates"]
    end
    
    subgraph phase3["ğŸš€ Phase 3: Production"]
        real["Real Data<br/>â€¢ ASAN KÉ™nd API<br/>â€¢ OAuth 2.0<br/>â€¢ Federated Learning"]
    end
    
    phase1 -->|"6 months"| phase2 -->|"12 months"| phase3
    
    style phase1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px,color:#1b5e20
    style phase2 fill:#fff9c4,stroke:#f9a825,color:#5d4037
    style phase3 fill:#e1f5fe,stroke:#0288d1,color:#01579b
```

### Hot-Swap Interface

```python
# src/yonca/sidecar/data_adapter.py
# Prepared for seamless Phase 2 transition

class DataAdapter(Protocol):
    """Interface for swappable data sources."""
    def get_farm_profile(self, farm_id: str) -> FarmProfile: ...
    def get_weather(self, region: str, days: int) -> list[WeatherData]: ...
    def get_soil_data(self, farm_id: str) -> SoilData: ...
```

---

## âœ… Logical Accuracy Framework

### Target: â‰¥90% Accuracy

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
flowchart LR
    subgraph pipeline["Accuracy Assurance Pipeline"]
        llm["ğŸ¤– LLM Output<br/><i>0.5 base</i>"]
        validate["ğŸ“š Rulebook<br/>Validator<br/><i>+0.4 match</i>"]
        resolve["âš–ï¸ Conflict<br/>Resolver<br/><i>+0.1 multi</i>"]
        score["ğŸ¯ Final Score<br/><i>â‰¥0.7 accept</i>"]
        
        llm --> validate --> resolve --> score
    end
    
    style validate fill:#fff9c4,stroke:#f9a825,stroke-width:2px,color:#5d4037
    style score fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
```

### Scoring Logic

| Component | Score Impact | Condition |
|:----------|:-------------|:----------|
| Base LLM confidence | 0.5 | Always |
| Rule match bonus | +0.4 | LLM matches rulebook |
| Multi-rule agreement | +0.1 | Multiple rules agree |
| No coverage | Ã—0.7 | No applicable rules |
| Contradiction | Ã—0.5 | LLM conflicts with rules |

### Example Validation Flow

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
flowchart TB
    subgraph query["ğŸ“ User Query"]
        q["Torpaq nÉ™mliyi 25%,<br/>bu gÃ¼n suvarmaq lazÄ±mdÄ±r?"]
    end
    
    subgraph step1["â‘  LLM Generation"]
        out["BÉ™li, dÉ™rhal suvarma lazÄ±mdÄ±r.<br/>SÉ™hÉ™r tezdÉ™n suvarÄ±n."]
        conf1["Base: 0.5"]
    end
    
    subgraph step2["â‘¡ Rulebook Check"]
        rule["âœ… AZ-IRR-001 triggered:<br/>moisture < 30% â†’ irrigate"]
        conf2["Rule: 0.95<br/>Bonus: +0.40"]
    end
    
    subgraph step3["â‘¢ Context Validation"]
        check1["Rain expected? âŒ No âœ“"]
        check2["Temp extreme? âŒ No âœ“"]
        no_conflict["No conflicts"]
    end
    
    subgraph step4["â‘£ Final Score"]
        calc["0.5 + 0.40 = 0.90"]
        status["âœ… HIGH CONFIDENCE"]
        cite["Matches AZ-IRR-001:<br/>Critical Low Moisture"]
    end
    
    query --> step1 --> step2 --> step3 --> step4
    
    style step4 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
```

---

## ğŸ“¡ API Schema

### REST Endpoints

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph LR
    subgraph core["ğŸ”Œ Core Endpoints"]
        chat["POST /yonca-ai/chat<br/><i>Main advisory endpoint</i>"]
        rec["POST /recommendations<br/><i>Get AI advice</i>"]
        status["GET /status<br/><i>Service health</i>"]
        caps["GET /capabilities<br/><i>Inference mode</i>"]
    end
    
    subgraph graph_ep["ğŸ§  LangGraph"]
        threads["GET /threads/{id}<br/><i>Session state</i>"]
        history["GET /threads/{id}/history<br/><i>Conversation memory</i>"]
    end
    
    subgraph rules_ep["ğŸ“š Rulebook"]
        rulebook["GET /rulebook<br/><i>All rules</i>"]
        cats["GET /rulebook/categories<br/><i>Categories</i>"]
    end
    
    subgraph admin["âš™ï¸ Admin"]
        mode["POST /mode/{mode}<br/><i>Switch mode</i>"]
        audit["GET /audit<br/><i>PII audit log</i>"]
        health["GET /health<br/><i>Health check</i>"]
    end
```

**Base URL:** `/api/v1/sidecar`

| Endpoint | Method | Description |
|:---------|:-------|:------------|
| `/yonca-ai/chat` | POST | **Primary streaming chat endpoint** (SSE) |
| `/recommendations` | POST | Get AI recommendations (batch) |
| `/threads/{thread_id}` | GET | Retrieve LangGraph session state |
| `/threads/{thread_id}/history` | GET | Get conversation memory |
| `/status` | GET | Service health & stats |
| `/capabilities` | GET | Current inference mode |
| `/models` | GET | Available model info |
| `/mode/{mode}` | POST | Switch inference mode |
| `/rulebook` | GET | Get agronomy rules |
| `/rulebook/categories` | GET | Rule categories |
| `/audit` | GET | PII audit summary |
| `/health` | GET | Health check |

### Request Schema (POST /recommendations)

```json
{
  "farm_id": "string (required)",
  "region": "string (required, e.g., 'Aran')",
  "farm_type": "string (required: wheat|vegetable|orchard|livestock|mixed)",
  "crops": ["string"],
  "area_hectares": "number (>0)",
  "soil_type": "string (clay|sandy|loamy|silty)",
  "soil_moisture_percent": "integer (0-100)",
  "temperature_max": "number (Â°C)",
  "precipitation_expected": "boolean",
  "query": "string (user question in Azerbaijani/English)",
  "language": "string (default: 'az')",
  "inference_mode": "string (standard|lite|offline)"
}
```

### Response Schema

```json
{
  "request_id": "string",
  "farm_id": "string",
  "recommendations": [
    {
      "id": "string",
      "type": "irrigation|fertilization|pest_control|...",
      "priority": "critical|high|medium|low",
      "confidence": 0.92,
      "title": "string",
      "title_az": "string",
      "description": "string",
      "description_az": "string",
      "source": "llm|rulebook|hybrid",
      "rule_id": "AZ-IRR-001"
    }
  ],
  "overall_confidence": 0.90,
  "accuracy_score": 0.92,
  "validation_notes": ["Matches rule AZ-IRR-001"],
  "inference_mode": "standard",
  "processing_time_ms": 245
}
```

---

## ğŸš€ Strategic Enhancements

### Five Enhancement Modules

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
graph TB
    subgraph enhancements["ğŸ¯ Strategic Enhancement Modules"]
        direction TB
        
        subgraph row1["Input Processing"]
            expert["ğŸ‘¨â€ğŸ”¬ Agronomist<br/>in-the-Loop"]
            dialect["ğŸ—£ï¸ Dialect<br/>Handler"]
            temporal["â° Temporal<br/>State Mgmt"]
        end
        
        subgraph core["Core RAG Engine"]
            rag["ğŸ¤– RAG + Rulebook"]
        end
        
        subgraph row2["Output Enhancement"]
            trust["âœ… Trust Score<br/>& Citations"]
            twin["ğŸŒ± Digital Twin<br/>Simulator"]
            response["ğŸ“¦ Enhanced<br/>API Response"]
        end
        
        row1 --> core --> row2
    end
    
    style expert fill:#e1bee7,stroke:#7b1fa2,color:#4a148c
    style dialect fill:#b2dfdb,stroke:#00796b,color:#004d40
    style temporal fill:#ffccbc,stroke:#e64a19,color:#bf360c
    style trust fill:#c8e6c9,stroke:#2e7d32,color:#1b5e20
    style twin fill:#bbdefb,stroke:#1565c0,color:#0d47a1
```

### Module Summary

| Module | Location | Purpose |
|:-------|:---------|:--------|
| **Agronomist-in-the-Loop** | `sidecar/validation.py` | 3-tier expert validation system |
| **Dialect Handler** | `sidecar/dialect.py` | Azerbaijani regional term normalization |
| **Temporal State** | `sidecar/temporal.py` | Farm timeline memory & context |
| **Trust & Citations** | `sidecar/trust.py` | Confidence breakdown & source citations |
| **Digital Twin** | `sidecar/digital_twin.py` | Farm simulation engine |

### Validation Tiers

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryTextColor': '#1a1a1a', 'lineColor': '#424242'}}}%%
flowchart LR
    subgraph tier1["ğŸŸ¢ Tier 1: Automatic"]
        auto["Pre-approved<br/>Rules match<br/>>90% confidence"]
        badge1["âœ… Expert Verified"]
    end
    
    subgraph tier2["ğŸŸ¡ Tier 2: Async"]
        queue["Expert Queue<br/><24h review<br/>High priority"]
        badge2["â³ Pending Review"]
    end
    
    subgraph tier3["ğŸ”´ Tier 3: Sync"]
        block["Real-time<br/>approval required<br/>Critical advice"]
        badge3["ğŸ”’ Blocked"]
    end
    
    auto --> badge1
    queue --> badge2
    block --> badge3
    
    style tier1 fill:#c8e6c9,stroke:#2e7d32,color:#1b5e20
    style tier2 fill:#fff9c4,stroke:#f9a825,color:#5d4037
    style tier3 fill:#ffcdd2,stroke:#c62828,color:#b71c1c
```

### Digital Twin Simulation

```mermaid
pie showData
    title Simulation Modes - Yield Impact
    "OPTIMAL (125%)" : 125
    "BASELINE (100%)" : 100
    "DROUGHT_STRESS (65%)" : 65
    "PEST_OUTBREAK (70%)" : 70
    "WORST_CASE (40%)" : 40
```

---

## â“ Technical Discovery: Questions for Digital Umbrella IT

With the LangGraph architecture, our integration requirements are well-defined. These questions will finalize the deployment strategy:

### Communication Protocol

| Question | Why It Matters |
|:---------|:--------------|
| **Do you use WebSockets or SSE for current chat features?** | Determines how we stream the agent's reasoning in real-time |
| **What is the mobile framework? (Flutter/React Native/Native)** | Affects SSE client implementation |

### State Management Strategy

| Question | Options |
|:---------|:--------|
| **Where should AI conversation state (memory) be stored?** | **Option A:** Our module's database (Redis/PostgreSQL) â€” Simpler integration<br/>**Option B:** Passed back via API â€” You control the data |

### Audit & Visualization

| Question | Capability Offered |
|:---------|:-------------------|
| **Can we provide LangGraph Studio visualization to your agronomists?** | Allows non-technical staff to audit AI decision paths without reading code |
| **Do you require full audit logs of all AI decisions?** | We can provide complete reasoning traces for compliance |

### Infrastructure

| Question | Impact |
|:---------|:-------|
| **Hosting preference: Your infrastructure vs. standalone Docker?** | Affects deployment complexity and data residency |
| **Expected concurrent users during peak farming season?** | Determines scaling strategy (horizontal vs. vertical) |

---

## ğŸ”§ Deployment Guide

### Quick Start

```bash
# 1. Install dependencies
poetry install --all-extras

# 2. Start Ollama with Qwen2.5
ollama pull qwen2.5:7b

# 3. Run Yonca with Sidecar
python -m yonca.startup
```

### Environment Variables

```bash
# .env file
YONCA_DEBUG=false
YONCA_DEFAULT_LANGUAGE=az
YONCA_RECOMMENDATION_CONFIDENCE_THRESHOLD=0.7

# Ollama
OLLAMA_HOST=http://localhost:11434

# LangGraph Configuration
LANGGRAPH_CHECKPOINT_BACKEND=postgres  # memory | redis | postgres
LANGGRAPH_POSTGRES_URI=postgresql://user:pass@localhost:5432/yonca
LANGGRAPH_REDIS_URL=redis://localhost:6379
LANGGRAPH_ENABLE_STREAMING=true
LANGGRAPH_MAX_RETRIES=3

# Sidecar
SIDECAR_INFERENCE_MODE=auto  # auto|standard|lite|offline
SIDECAR_ENABLE_AUDIT_LOG=true
SIDECAR_GGUF_MODEL=qwen2.5-7b-q4
```

### Docker Deployment

```bash
# Single command deployment
docker run -d \
  --name yonca-ai-sidecar \
  -p 8000:8000 \
  -e LANGGRAPH_CHECKPOINT_BACKEND=redis \
  -e LANGGRAPH_REDIS_URL=redis://redis:6379 \
  zekalab/yonca-sidecar:latest
```

### Docker Compose (Full Stack)

```yaml
version: '3.8'
services:
  yonca-sidecar:
    image: zekalab/yonca-sidecar:latest
    ports:
      - "8000:8000"
    environment:
      - LANGGRAPH_CHECKPOINT_BACKEND=postgres
      - LANGGRAPH_POSTGRES_URI=postgresql://yonca:secret@postgres:5432/yonca
    depends_on:
      - postgres
      - ollama

  postgres:
    image: postgres:16-alpine
    environment:
      - POSTGRES_USER=yonca
      - POSTGRES_PASSWORD=secret
      - POSTGRES_DB=yonca
    volumes:
      - pgdata:/var/lib/postgresql/data

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama:/root/.ollama

volumes:
  pgdata:
  ollama:
```

### Edge Deployment

```python
from yonca.sidecar.lite_inference import EdgeDeploymentConfig

config = EdgeDeploymentConfig(
    max_memory_mb=2000,
    has_gpu=False,
    expected_bandwidth_kbps=256,
    is_intermittent=True,
)
```

---

## ï¿½ Implementation Checklist

### Step 1: Graph Definition (Logic Layer)
- [ ] Define `FarmingState` TypedDict with all context fields
- [ ] Implement processing nodes: `Scenario_Analyzer`, `Recommendation_Engine`, `Safety_Guardrail`
- [ ] Configure conditional edges for risk-based routing
- [ ] Set up `MemorySaver` checkpointer for session persistence

### Step 2: Safety First Module
- [ ] Build Redline Scanner node for zero-real-data validation
- [ ] Implement PII detection patterns for Azerbaijani data
- [ ] Configure retry loop for failed compliance checks
- [ ] Add audit logging for all blocked responses

### Step 3: FastAPI Bridge
- [ ] Wrap LangGraph in FastAPI endpoints
- [ ] Implement SSE streaming for real-time responses
- [ ] Add thread management for multi-session support
- [ ] Configure CORS for mobile app access

### Step 4: Deployment
- [ ] Dockerize the complete stack
- [ ] Document single `POST /yonca-ai/chat` endpoint
- [ ] Provide LangGraph Studio access for agronomist auditing
- [ ] Performance test with expected concurrent users

---

## ğŸ” Security Summary

### PII Protection Matrix

| Data Type | Treatment | Storage |
|:----------|:----------|:--------|
| Farmer Name | `[ÅÆXS_1]` | Never stored |
| Phone | `[TELEFON]` | SHA-256 hash only |
| GPS Coords | `[KOORDÄ°NAT]` | Region code only |
| Farm ID | `syn_abc123` | Token mapping |
| Soil/Weather | Passed through | No PII risk |
| **Chat History** | Anonymized in Checkpointer | Thread ID only |

---

<div align="center">

**ğŸ“„ Document:** `03-ARCHITECTURE.md`  
**â¬…ï¸ Previous:** [02-GAP-ANALYSIS.md](02-GAP-ANALYSIS.md) â€” Client Discovery  
**ğŸ  Index:** [README.md](README.md) â€” Documentation Hub

---

*ZekaLab â€” Headless Intelligence as a Service*  
*Built with ğŸŒ¿ LangGraph for Azerbaijan's agricultural future*

</div>
