# ğŸ§ª ALEM Testing Strategy

> **Purpose:** Evaluation framework and test suites for ALEM quality assurance.

---

## ğŸ“‹ Implementation Status

| Test Category | Status | Location |
|:--------------|:-------|:---------|
| Unit Tests | âœ… Implemented | `tests/unit/` â€” 6 test files |
| Integration Tests | âœ… Implemented | `tests/integration/test_llm_integration.py` |
| Evaluation Tests | âŒ TODO | `tests/evaluation/` â€” empty |
| Golden Dataset | âŒ TODO | Schema defined below |

**Priority:** Evaluation suite is **HIGH priority** for production readiness.

---

## ğŸ¯ Testing Philosophy

```mermaid
%%{init: {'theme': 'neutral'}}%%
mindmap
  root((ğŸ§ª Testing))
    ğŸ¯ Accuracy
      Benchmark dataset
      Expert validation
    ğŸ›¡ï¸ Safety
      Hallucination detection
      PII leakage tests
    âš¡ Performance
      Latency benchmarks
    ğŸ”„ Regression
      Automated gates
```

| Principle | Implementation |
|:----------|:---------------|
| **Test Before Deploy** | No prompt changes without passing evaluation |
| **Expert-in-the-Loop** | Agronomists validate golden dataset |
| **Fail Fast** | Automated gates block regressions |

---

## ğŸ“Š Golden Dataset Structure

```mermaid
%%{init: {'theme': 'neutral'}}%%
erDiagram
    TEST_CASE {
        string case_id PK "TC-IRR-001"
        string category "irrigation"
        string difficulty "medium"
        json context "farm, weather, user"
        string query_az "Azerbaijani question"
        json expected "must_include, must_not"
    }
```

### Category Coverage (Planned)

| Category | # Cases | Priority |
|:---------|:-------:|:--------:|
| ğŸ’§ Irrigation | 25 | ğŸ”´ Critical |
| ğŸ§ª Fertilization | 20 | ğŸ”´ Critical |
| ğŸ› Pest/Disease | 25 | ğŸ”´ Critical |
| ğŸŒ¾ Harvest | 15 | ğŸŸ¡ High |
| â“ Edge Cases | 20 | ğŸ”´ Critical |
| **Total** | **105+** | |

### Test Case Schema

```json
{
  "case_id": "TC-IRR-001",
  "category": "irrigation",
  "difficulty": "medium",
  "context": {
    "farm_profile": {"crop": "pomidor", "area_ha": 5},
    "weather": {"temp_c": 32, "humidity": 40},
    "date": "2026-07-15"
  },
  "query": {
    "text_az": "PomidorlarÄ± nÉ™ vaxt suvarmalÄ±yam?",
    "intent": "irrigation_advice"
  },
  "expected": {
    "must_include_concepts": ["sÉ™hÉ™r", "axÅŸam", "rÃ¼tubÉ™t"],
    "must_not_include": ["qÄ±ÅŸ", "don"],
    "rule_triggers": ["RULE_IRR_001"]
  }
}
```

---

## ğŸš€ Running Tests

```powershell
# Unit tests
pytest tests/unit/ -v --tb=short

# Integration tests
pytest tests/integration/ -v

# All tests with coverage
pytest tests/ -v --cov=src/ALÄ°M
```

---

## ğŸ“‹ TODO: Evaluation Suite

1. Create `tests/evaluation/` test files
2. Build golden dataset (105+ cases)
3. Implement evaluation metrics:
   - Semantic similarity scoring
   - Concept inclusion checking
   - Rule trigger verification
4. Add to CI pipeline
