# ğŸ“Š ALEM Observability Guide

> **Purpose:** Logging, tracing, and monitoring for ALEM production visibility.

---

## âœ… Implementation Status

| Component | Status | Location |
|:----------|:-------|:---------|
| **Langfuse Integration** | âœ… Implemented | `src/yonca/observability/langfuse.py` |
| Docker Compose | âœ… Configured | `docker-compose.local.yml` |
| Agent Callbacks | âœ… Wired | `src/yonca/agent/graph.py` |
| Prometheus Metrics | â³ Not implemented | Future |

---

## ğŸ” Langfuse: Self-Hosted LLM Observability

**Langfuse** provides 100% data residency â€” all traces stay within your infrastructure.

### Quick Start

```bash
# 1. Start Langfuse
docker-compose -f docker-compose.local.yml up langfuse-server langfuse-db -d

# 2. Open http://localhost:3001, create account, get API keys

# 3. Add to .env:
YONCA_LANGFUSE_SECRET_KEY=sk-lf-...
YONCA_LANGFUSE_PUBLIC_KEY=pk-lf-...
YONCA_LANGFUSE_HOST=http://localhost:3001
```

### Dashboard Features

- ğŸ” Full LangGraph node tracing with timing
- ğŸ’° Token/cost tracking per model
- ğŸ“Š Session grouping by thread_id
- ğŸ‘¥ Per-user analytics
- ğŸ“ Prompt versioning
- âš¡ Evaluation datasets

### Architecture

```mermaid
%%{init: {'theme': 'neutral'}}%%
flowchart LR
    subgraph app["ğŸ§  ALEM Agent"]
        graph["LangGraph"]
        llm["LLM Provider"]
    end

    subgraph observe["ğŸ“Š Langfuse (:3001)"]
        traces["Traces"]
        sessions["Sessions"]
        costs["Cost Tracking"]
    end

    graph --> |"Callbacks"| traces
    llm --> |"Token usage"| costs

    style observe fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
```

---

## ğŸ“ˆ Metric Categories

| Category | Examples | Status |
|:---------|:---------|:-------|
| **LLM Metrics** | Token usage, latency, costs | âœ… Langfuse |
| **Session Metrics** | Thread count, user activity | âœ… Langfuse |
| **System Metrics** | CPU, memory, GPU | â³ Prometheus (future) |
| **Business Metrics** | Task completion, satisfaction | â³ Custom (future) |

---

## ğŸ”§ Configuration

```python
# src/yonca/config.py
class Settings:
    langfuse_enabled: bool = True
    langfuse_host: str = "http://localhost:3001"
    langfuse_secret_key: str | None = None
    langfuse_public_key: str | None = None
    langfuse_sample_rate: float = 1.0
    langfuse_debug: bool = False
```

---

## ğŸ“‹ Future: Prometheus/Grafana

Optional infrastructure monitoring (lower priority than Langfuse):

| Metric | Type | Use |
|:-------|:-----|:----|
| `alem_requests_total` | Counter | Request rate |
| `alem_request_duration_seconds` | Histogram | Latency |
| `alem_active_sessions` | Gauge | Concurrent users |
| `alem_errors_total` | Counter | Error tracking |
