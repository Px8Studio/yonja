Take your LLM apps to the next level with Chainlit!
Part 2
Tituslhy
Tituslhy

Follow
15 min read
¬∑
May 5, 2025
105




Press enter or click to view image in full size

Typing‚Äôs too slow, use a mic! Image generated by ChatGPT using author‚Äôs image
In my previous article, I wrote about how to build a generative AI assistant with Google Map capabilities from the ground up. We did some amazing things:

Built an app to support the entire chat lifecycle: login, start chat, handle incoming messages, stop tasks, resume chats, and logout
Added chat starters to help users kick off conversations
Created chat profiles to inject different system prompts for our LLM agent
Implemented chat settings to allow advanced configuration (e.g. choosing different LLMs or tweaking temperature)
Built a custom Canvas using Google Maps, wrapped into a LlamaIndex Function Agent
Press enter or click to view image in full size

We wrapped up steps 1‚Äì6 above on our home page. In this article, we‚Äôre taking things to the next level (actually, several levels): adding image generation, AutoRAG, audio support, and Model Context Protocol (MCP) capabilities.

As before, all my codes can be found in my GitHub repository.

1. Adding image generation capabilities via Chainlit Commands üñºÔ∏è
The ChatGPT interface is able to generate images if you specify for it to do so. We can configure this in Chainlit too. Let‚Äôs add the following into app.py:

# <earlier imports>

commands = [
    {"id": "Picture", "icon": "image", "description": "Use DALL-E"},
]

@cl.on_chat_start
async def start():
    """Handler for chat start events. Sets session variables."""

    await cl.context.emitter.set_commands(commands)
    ... # <rest of your on_chat_start logic>
In Chainlit, commands are defined as a list of dictionaries. Each dictionary must include:

id: the name of the command (e.g. "Picture")
icon: a Lucide icon name
description: a short label for what the command does
These commands are set at the beginning of each chat using set_commands(). Once your app is restarted, a Commands button will appear in the message bar, letting users choose from your list ‚Äî as shown below:

Press enter or click to view image in full size

Not so fast ‚Äî clicking the command won‚Äôt do anything yet. We haven‚Äôt wired up the image generation logic. Since Chainlit sends commands via the message bar, we‚Äôll need to modify our @cl.on_message handler. Add this code snippet to your app.py:

# <earlier imports and on_chat_start code>
from openai import AsyncOpenAI

openai_client = AsyncOpenAI(api_key="...")

@cl.on_message
async def on_message(message: cl.Message):
    if message.command == "Picture":
        response = await openai_client.images.generate(
            model="dall-e-3",
            prompt = message.content,
            size = "1024x1024"
        )
        logger.info(f"Image generated. Reponse: {response}")
        image_url = response.data[0].url
        elements = [cl.Image(url=image_url)]
        await cl.Message(f"Here's what I generated for **{message.content}**", elements=elements).send()
    else:
        # your on_message logic here.
Chainlit automatically detects commands attached to a message, so you can easily branch your logic based on message.command.

In this case, when we detect the "Picture" command, we treat the user‚Äôs message as the prompt and send it to OpenAI‚Äôs DALL¬∑E 3 model. Once the image is generated, we extract the URL and use Chainlit‚Äôs cl.Image() to render it directly in the chat interface.

Press enter or click to view image in full size
Press enter or click to view image in full size
‚ú® Voila! You now have image generation working inside your chat app ‚Äî and it looks fantastic.

2. Audio capabilities üéôÔ∏è
This next section takes everything we‚Äôve built so far (already ‚ÄúMasterChef‚Äù level) and elevates it to ‚ÄúMichelin Star Chef‚Äù territory.

At its core, enabling audio capabilities means supporting both speech-to-text and text-to-speech:

For speech-to-text, we‚Äôll use OpenAI‚Äôs Whisper (yes, we‚Äôre already using GPT-4o-mini and DALL¬∑E 3 ‚Äî OpenAI really is a full-stack LLM shop).
For text-to-speech, we‚Äôll use my favorite service: ElevenLabs. It offers high-quality voices (even Singaporean English!) and a generous free tier. Grab an API key to get started.
üì¶ Setup Code
Here‚Äôs our initial setup for handling audio input:

## Setup
import os
from dotenv import load_dotenv, find_dotenv
import chainlit as cl
import numpy as np

_ = load_dotenv(find_dotenv())

SILENCE_THRESHOLD = 3500  # Adjust based on your audio level (e.g., lower for quieter audio)
SILENCE_TIMEOUT = 1300.0  # Seconds of silence to consider the turn finished
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
ELEVENLABS_VOICE_ID = os.getenv("ELEVENLABS_VOICE_ID")

@cl.on_audio_start
async def on_audio_start():
    """Handler to manage mic button click event"""

    cl.user_session.set("silent_duration_ms", 0)
    cl.user_session.set("is_speaking", False)
    cl.user_session.set("audio_chunks", [])

    user = cl.user_session.get("user")
    logger.info(f"{user} is starting an audio stream...")
    return True
The on_audio_start handler instantiates the audio-specific session variables when audio recording starts. Now let‚Äôs handle the actual audio stream:

@cl.on_audio_chunk
async def on_audio_chunk(chunk: cl.InputAudioChunk):
    """Handler function to manage audio chunks
    Source: Chainlit Cookbook
    """

    audio_chunks = cl.user_session.get("audio_chunks")

    if audio_chunks is not None:
        audio_chunk = np.frombuffer(chunk.data, dtype=np.int16)
        audio_chunks.append(audio_chunk)

    # If this is the first chunk, initialize timers and state
    if chunk.isStart:
        cl.user_session.set("last_elapsed_time", chunk.elapsedTime)
        cl.user_session.set("is_speaking", True)
        return

    last_elapsed_time = cl.user_session.get("last_elapsed_time")
    silent_duration_ms = cl.user_session.get("silent_duration_ms")
    is_speaking = cl.user_session.get("is_speaking")

    # Calculate the time difference between this chunk and the previous one
    time_diff_ms = chunk.elapsedTime - last_elapsed_time
    cl.user_session.set("last_elapsed_time", chunk.elapsedTime)

    # Compute the RMS (root mean square) energy of the audio chunk
    audio_energy = audioop.rms(
        chunk.data, 2
    )  # Assumes 16-bit audio (2 bytes per sample)

    if audio_energy < SILENCE_THRESHOLD:
        # Audio is considered silent
        silent_duration_ms += time_diff_ms
        cl.user_session.set("silent_duration_ms", silent_duration_ms)
        if silent_duration_ms >= SILENCE_TIMEOUT and is_speaking:
            cl.user_session.set("is_speaking", False)
            await process_audio()
    else:
        # Audio is not silent, reset silence timer and mark as speaking
        cl.user_session.set("silent_duration_ms", 0)
        if not is_speaking:
            cl.user_session.set("is_speaking", True)
üß† How This Works
As audio chunks are streamed, we use numpy to convert the byte buffer into a numeric array. We also use Python‚Äôs audioop.rms() to calculate the energy level of each chunk. If the energy falls below a threshold for a long enough time (SILENCE_TIMEOUT), we assume the user has stopped speaking ‚Äî and trigger audio processing.

Before we dive into Whisper and ElevenLabs integration, let‚Äôs abstract the message generation logic from earlier into a reusable function:

async def generate_answer(query: str):
    agent = cl.user_session.get("agent")
    memory = cl.user_session.get("memory")
    chat_history = memory.get()
    msg = cl.Message("", type="assistant_message")

    context = cl.user_session.get("context")
    handler = agent.run(
        query,
        chat_history = chat_history,
        ctx = context
    )
    async for event in handler.stream_events():
        if isinstance(event, AgentStream):
            await msg.stream_token(event.delta)
        elif isinstance(event, ToolCall):
            with cl.Step(name=f"{event.tool_name} tool", type="tool"):
                continue

    response = await handler
    await msg.send()
    memory.put(
        ChatMessage(
            role = MessageRole.USER,
            content= query
        )
    )
    memory.put(
        ChatMessage(
            role = MessageRole.ASSISTANT,
            content = str(response)
        )
    )
    cl.user_session.set("memory", memory)
    return msg
This lets us reuse the same flow for both text and audio inputs. Now let‚Äôs write our function to transform text to speech:

import io
import httpx

@cl.step(type="tool")
async def text_to_speech(text: str, mime_type: str):
    """Our main text to speech function
    Source: Chainlit Cookbook, ElevenLabs Documentation
    """

    CHUNK_SIZE = 1024

    url = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}"

    headers = {
        "Accept": mime_type,
        "Content-Type": "application/json",
        "xi-api-key": ELEVENLABS_API_KEY,
    }

    data = {
        "text": text,
        "model_id": "eleven_multilingual_v2",
        "voice_settings": {"stability": 0.5, "similarity_boost": 0.5},
    }

    async with httpx.AsyncClient(timeout=25.0) as client:
        response = await client.post(url, json=data, headers=headers)
        response.raise_for_status()  # Ensure we notice bad responses

        buffer = io.BytesIO()
        buffer.name = f"output_audio.{mime_type.split('/')[1]}"

        async for chunk in response.aiter_bytes(chunk_size=CHUNK_SIZE):
            if chunk:
                buffer.write(chunk)

        buffer.seek(0)
        return buffer.name, buffer.read()
We are essentially posting a request to our ElevenLabs client to get the audio as a stream of bytes which we return to render as a downloadable element along with our message. The speech-to-text is even easier because it‚Äôs just posting the entire speech to OpenAI:

from openai import AsyncOpenAI

openai_client = AsyncOpenAI(api_key="...")

@cl.step(type="tool")
async def speech_to_text(audio_file):
    response = await openai_client.audio.transcriptions.create(
        model="whisper-1", file=audio_file, language="en",
    )
    return response.text
The only thing that we need to take note of is the OpenAI invocation of Whisper requires an audio_file that we must first construct.

üìå Tip: Specify a language to improve accuracy. Whisper thinks I speak Malay at times.

And now to finally process the audio stream head on using all our previous functions:

async def process_audio():
    """ Processes the audio buffer from the session
    Source: Chainlit Cookbook
    """

    if audio_chunks := cl.user_session.get("audio_chunks"):
        # Concatenate all chunks
        concatenated = np.concatenate(list(audio_chunks))

        # Create an in-memory binary stream
        wav_buffer = io.BytesIO()

        # Create WAV file with proper parameters
        with wave.open(wav_buffer, "wb") as wav_file:
            wav_file.setnchannels(1)  # mono
            wav_file.setsampwidth(2)  # 2 bytes per sample (16-bit)
            wav_file.setframerate(24000)  # sample rate (24kHz PCM)
            wav_file.writeframes(concatenated.tobytes())

        # Reset buffer position
        wav_buffer.seek(0)
        cl.user_session.set("audio_chunks", [])

    frames = wav_file.getnframes()
    rate = wav_file.getframerate()

    duration = frames / float(rate)
    if duration <= 1.71:
        print("The audio is too short, please try again.")
        return

    audio_buffer = wav_buffer.getvalue()
    input_audio_el = cl.Audio(content=audio_buffer, mime="audio/wav")
    whisper_input = ("audio.wav", audio_buffer, "audio/wav")
    transcription = await speech_to_text(whisper_input) #send to Whisper

    user = cl.user_session.get("user")
    logger.info(f"Received message: '{transcription}' from {user}")

    await cl.Message(
        author="You",
        type="user_message",
        content=transcription,
        elements=[input_audio_el],
    ).send()

    ## Now to answer the question
    msg = await generate_answer(transcription) #send to gpt-4o-mini

    # Send to ElevenLabs
    _, output_audio = await text_to_speech(msg.content, "audio/wav")

    output_audio_el = cl.Audio(
        auto_play=True,
        mime="audio/wav",
        content=output_audio,
    )
    msg.elements=[output_audio_el]
    await msg.update()
Here‚Äôs what‚Äôs happening

We first take all the audio chunks and write a .wav file as an element to be appended to a Chainlit message.
Since we‚Äôve read the audio buffer in generating this wav file we need to reset the buffer to zero, and wrap the file format and the buffer into a tuple for the OpenAI Whisper Client. We then send everything to OpenAI to get the transcription .
We send the transcription along with the .wav file as a message from the user ( author=‚ÄùYou‚Äùmakes the message pop up on the user‚Äôs side of the chat). This .wav file is downloadable and this step is purely for user experience ‚Äî so the user knows you get the question asked.
We then send the raw transcription to the LLM agent to generate an answer, and send the answer to ElevenLabs to generate the audio file, which we then wrap as a Chainlit Audio element and append it to the LLM agent‚Äôs reply. We send the reply and the audio element as a message.
Wow, that was a lot of code! I wish I could say I wrote it all from scratch, but full credit goes to the Chainlit team ‚Äî this was adapted from their excellent audio cookbook.

Get Tituslhy‚Äôs stories in your inbox
Join Medium for free to get updates from this writer.

Enter your email
Subscribe
I spent quite a bit of time studying how it worked and then tailoring it to use Azure‚Äôs Text-To-Speech service at work. It was a great way to learn the internals of audio management while bending it to fit real-world needs (and impress your bosses). Once you‚Äôve added all the code above into app.py, restart the app and watch the magic happen:


Was that magic or what!

3. üîó Auto RAG capabilities ‚Äî upload any file and chat with it!
This is the bread and butter of any LLM application ‚Äî the ability to RAG files on demand (instead of reading them). Clicking on the file attach icon pins the file as a Chainlit element with the message you send. We just need to edit our on_message code to cater for this

from llama_index.core import SimpleDirectoryReader
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core.tools import QueryEngineTool

@cl.on_message
async def on_message(message: cl.Message):
    ... # pre-processing code
    if message.command == "Picture":
        ... # image generation code
    else:
        if len(message.elements) > 0:
           await cl.Message("Processing files").send()
           filepaths = [file.path for file in message.elements]
           filenames = [file.name for file in message.elements]
           documents = SimpleDirectoryReader(input_files=filepaths).load_data()

           ## Ingest documents into in-memory Vector Database.
           index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
           await cl.Message("Processed uploaded files").send()

           openai_llm = cl.user_session.get("llm")
           name = openai_llm.complete(f"Based on these filenames, come up with a short, concise name that describes these documents. For example 'MBA Value Analysis'. Do not return any '.pdf' or file extensions, just the name. Filenames: {', '.join(filenames)}")
           description = openai_llm.complete(f"Based on these filenames, come up with a consolidated description that describes these documents. For example 'Answers questions about animals'. Filenames: {', '.join(filenames)}")
           await cl.Message(f"Uploaded document/s follow the theme: {name}. Here's the general description of the document/s uploaded: {description}").send()

           tool = QueryEngineTool.from_defaults(
                query_engine=index.as_query_engine(similarity_top_k=8, llm=openai_llm),
                name = "_".join(str(name).split(" ")),
                description=str(description)
            )
            agent_tools = cl.user_session.get("agent_tools", [])
            agent_tools.append(tool)

            agent = FunctionAgent(tools=agent_tools, llm=openai_llm)
            cl.user_session.set("agent", agent)
            cl.user_session.set("agent_tools", agent_tools)

     await generate_answer(message.content) #use our new generate_answer function from before
When Chainlit detects uploaded files, it temporarily stores them in a .files directory and exposes the paths via message.elements. We then:

Use SimpleDirectoryReader to load the files (works with PDFs, PPTs, Word, Excel, even audio/video if Whisper is available).
Create a temporary index for similarity search.
Dynamically generate metadata (name and description) for the tool using the OpenAI LLM.
Convert the index to a query engine (with just one line of code ‚Äî thank you LlamaIndex!) and register a QueryEngineTool as a tool in this user session for the agent to use.
üìå Tip: Swap SimpleDirectoryReader with LlamaParse for smarter document chunking and cleaner structure.

Press enter or click to view image in full size

Note that the uploaded files are tied to the current user session. Once the chat ends, those files (and their corresponding tools) are lost unless you implement persistence.

Note that the uploaded files are tied to the current user session. Once the chat ends, those files (and their corresponding tools) are lost unless you implement persistence.

@cl.on_chat_resume
async def on_chat_resume(thread: ThreadDict):
    ... #your on chat resume logic
    await cl.Message("Chat resumed. Do note that previously uploaded documents will not be available in this chat and must be uploaded again").send()
4. üîå Model Context Protocol: Chat with all tools!
So far, we‚Äôve learned how to:

Define tools statically in code (google maps).
Dynamically register tools to an agent during a session (auto RAG).
Now it‚Äôs time to level up: let‚Äôs give our LLM agent the ability to access any available tools on demand based on user needs ‚Äî using the Model Context Protocol (MCP)! I‚Äôve previously written an article about MCPs ‚Äî feel free to check it out, but here‚Äôs the gist:

The value of an MCP lies in enabling dynamic tool availability at runtime. Your LLM can connect to or disconnect from external services like Jira or Confluence directly in chat ‚Äî enhancing user flexibility and keeping the frontend experience fluid.

Official vendors often provide Docker-based MCP servers ‚Äî or you can build your own. For this demo, let‚Äôs integrate Jira and allow our LLM to manage tickets in real time.

üõ† Step 1: Handle MCP Connections
When a user connects to an MCP server, we‚Äôll fetch the available tools, unpack them using LlamaIndex‚Äôs to_tool_list_async(), and update the agent accordingly:

@cl.on_mcp_connect
async def on_mcp_connect(connection):
    """Handler to connect to an MCP server.
    Lists tools available on the server and connects these tools to
    the LLM agent."""

    openai_llm = cl.user_session.get("llm")
    mcp_cache = cl.user_session.get("mcp_tool_cache", {})
    mcp_tools = cl.user_session.get("mcp_tools", {})
    agent_tools = cl.user_session.get("agent_tools", [])
    try:
        logger.info("Connecting to MCP")
        mcp_client = BasicMCPClient(connection.url)
        logger.info("Connected to MCP")
        mcp_tool_spec = McpToolSpec(client=mcp_client)
        logger.info("Unpacking tools")
        new_tools = await mcp_tool_spec.to_tool_list_async()
        for tool in new_tools:
            if tool.metadata.name not in mcp_tools:
                mcp_tools[tool.metadata.name] = tool
                mcp_cache[connection.name].append(tool.metadata.name)
        agent = FunctionAgent(
            tools=agent_tools + list(mcp_tools.values()),
            llm=openai_llm,
        )
        cl.user_session.set("agent", agent)
        cl.user_session.set("context", Context(agent))
        cl.user_session.set("mcp_tools", mcp_tools)
        cl.user_session.set("mcp_tool_cache", mcp_cache)
        await cl.Message(f"Connected to MCP server: {connection.name} on {connection.url}", type="assistant_message").send()

        await cl.Message(
            f"Found {len(new_tools)} tools from {connection.name} MCP server.", type="assistant_message"
        ).send()
    except Exception as e:
        await cl.Message(f"Error conecting to tools from MCP server: {str(e)}", type="assistant_message").send()
This ensures any newly discovered tools are merged into the session‚Äôs toolset.

üîå Step 2: Handle Disconnections
When a user disconnects from an MCP server, we‚Äôll cleanly remove its tools:

@cl.on_mcp_disconnect
async def on_mcp_disconnect(name: str):
    """Handler to handle disconnects from an MCP server.
    Updates tool list available for the LLM agent.
    """
    openai_llm = cl.user_session.get("llm")
    agent_tools = cl.user_session.get("agent_tools", [])
    mcp_tools = cl.user_session.get("mcp_tools", {})
    mcp_cache = cl.user_session.get("mcp_tool_cache", {})

    if name in mcp_cache:
        for tool_name in mcp_cache[name]:
            del mcp_tools[tool_name]
        del mcp_cache[name]

    # Update tools list in agent
    if len(mcp_tools)>0:
        agent = FunctionAgent(
            tools=agent_tools + list(mcp_tools.values()), #agent still has tools not removed
            llm=openai_llm,
        )
    else:
        agent = FunctionAgent(
            tools=agent_tools,
            llm=openai_llm,
        )
    cl.user_session.set("context", Context(agent))
    cl.user_session.set("mcp_tools", mcp_tools)
    cl.user_session.set("mcp_tool_cache", mcp_cache)
    cl.user_session.set("agent", agent)

    await cl.Message(f"Disconnected from MCP server: {name}", type="assistant_message").send()
üê≥ Step 3: Run Jira MCP via Docker
Jira‚Äôs MCP is available on Docker (do check with the official MCP tool page of the service you‚Äôre looking for), so all we need to do is get a Jira API token, pull the image and run it:

docker run --rm -p 9000:9000 \
  --env-file ./.env \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport sse --port 9000 -vv
Once running, click the plug icon in our Chainlit home page to connect.

Press enter or click to view image in full size
Press enter or click to view image in full size
Yup there are 26 tools!

Press enter or click to view image in full size

Now let‚Äôs have some fun! Let‚Äôs get GPT-4o-mini to check for active issues in our current sprint (note that you‚Äôll need to know at least the project key of your Jira project ‚Äî in my case it‚Äôs ‚ÄúJIRACHAT‚Äù)

Press enter or click to view image in full size

Since we‚Äôve already written all the features above, let‚Äôs resolve these issues!

Press enter or click to view image in full size

You can also tell the LLM to complete the sprint! What you get is a nice Jira home page that is clean indicating that the issues have been completed.

Press enter or click to view image in full size

PS: This MCP image also works for Confluence! So you can get your LLM to write sprint retrospectives on Confluence, or use this MCP to RAG Confluence pages.

It gets better ‚Äî deployment
Chainlit apps don‚Äôt just have to be deployed as standalone web applications. Chainlit apps can also be mounted as a copilot onto a website as a chat bubble, deployed on Teams, Slack or even Discord as a chatbot. The same app.py we built has incredible mileage ‚Äî you barely need to change a thing.

It must be said however, that you tend to lose control over the user experience when deploying Chainlit chatbots via the aforementioned alternatives ‚Äî because Chainlit must comply to the service requirements of these services. You do need administrative access to deploy on Teams, Slack and Discord which I don‚Äôt have so I‚Äôll just briefly demonstrate copilot deployment.

You‚Äôre going to have to know quite a bit of JavaScript and use the Chainlit CopilotFunction abstraction to shuttle arguments between the frontend and your Chainlit bot ‚Äî I‚Äôve done this at work with some struggle. The challenge comes from needing to authenticate the user at the main website first, which means that your app.py‚Äôs password_auth_callback will throw an error.

Instead of adding to our almost 600 line app.py, I‚Äôve elected to code a simple chatbot to quickly help illustrate the core concepts needed:

#simple_app.py

import chainlit as cl
from llama_index.llms.openai import OpenAI

llm = OpenAI('gpt-4o-mini', temperature=0)

@cl.on_message
async def on_message(message: cl.Message):
    reply = await llm.acomplete(message.content)
    response = await cl.Message(content=str(reply)).send()

    if cl.context.session.client_type == "copilot":
        fn = cl.CopilotFunction(
            name="test",
            args={"message": message.content, "response": response.content}
        )
        await fn.acall()
Yup it‚Äôs just a chatbot that answers questions ‚Äî we didn‚Äôt even implement memory and it doesn‚Äôt even have an on_chat_start!

If Chainlit detects that the context of the client‚Äôs session is copilot mode, the cl.CopilotFunction class creates a function call event that is sent from the Chainlit backend to the embedded Copilot widget on your website or application.

When you instantiate it with a name and args, and then invoke .acall(), Chainlit emits a chainlit-call-fn event to the front end, carrying those parameters. A corresponding JavaScript event listener on the host page picks up that event, executes the requested action (e.g., updating UI or fetching data), and returns a result via a callback ‚Äî which is then propagated back into your Chainlit app for further processing

In our index.html, be sure to add the codes in the script tags. This mounts the Chainlit widget as a little chat bubble and instantiates an event listener to pass the message to our Chainlit application.

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=<device-width>, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <!-- ... -->
  <script src="http://localhost:8001/copilot/index.js"></script>
  <script>
    window.mountChainlitWidget({
        chainlitServer: "http://localhost:8001",
      });
    window.addEventListener("chainlit-call-fn", (e) => {
      const { name, args, callback } = e.detail;
      if (name=="test"){
        callback(args.response); //send the results in this callback function
      }
    });
  </script>
  <!-- ... -->
</body>
</html>
Also, be sure to edit your .chainlit/config.toml file to allow the origin that your website is deployed on. This is annoying because ‚Äú*‚Äù typically results in a failure to fetch error so you have to be explicit.

allow_origins = ["http://127.0.0.1:5500"]
The result looks really good!

Press enter or click to view image in full size

You can even click the top right hand corner of the chat bubble to expand the chat window. Copilot mode supports all the advanced features we built earlier!

Press enter or click to view image in full size

Just be mindful to handle the authentication process in your Chainlit application on your website. The sticking point is passing authenticated user context from the host site into the Chainlit app ‚Äî especially since password_auth_callback doesn‚Äôt play nicely with copilot embeds.

If you manage to get auth working cleanly, do drop me a note ‚Äî I‚Äôd love to learn from your setup too!

Concluding thoughts
We‚Äôve covered a lot of ground! Developing LLM applications with Chainlit is genuinely enjoyable ‚Äî it offers a level of simplicity and flexibility that‚Äôs hard to beat. The mileage you can get out of a single app.py file is impressive. I hope this series has served as both a practical starting point and a useful cookbook for anyone looking to build with Chainlit.

Disclaimer: All opinions and interpretations are that of the writer, and not of MITB. I declare that I have full rights to use the contents published here, and nothing is plagiarized. I declare that this article is written by me and not with any generative AI tool such as ChatGPT. I declare that no data privacy policy is breached, and that any data associated with the contents here are obtained legitimately to the best of my knowledge. I agree not to make any changes without first seeking the editors‚Äô approval. Any violations may lead to this article being retracted from the publication.

Chainlit
Elevenlabs Ai
Llamaindex
LLM
Data Science
105



MITB For All
Published in MITB For All
296 followers
¬∑
Last published Jan 14, 2026
Tech contents related to AI, Analytics, Fintech, and Digital Transformation. Written by MITB Alumni; open-access for everyone.


Follow
Tituslhy
Written by Tituslhy
309 followers
¬∑
8 following
