# Quick Reference: Model Selection for Yonca AI

## ðŸ† The Gold Standard: 70B Parameter Class

> **For national-scale agricultural AI, 70B models are the "Goldilocks zone"**â€”smart enough for complex agronomic reasoning, efficient enough for high-end local hardware.

### Why 70B Over 8B?

| Capability | 8B Models | 70B Models (Gold Standard) |
|:-----------|:----------|:---------------------------|
| **Reasoning** | Single-step | Multi-step (soil pH + weather + crop stage) |
| **Azerbaijani** | Turkish leakage risk | Strong internal filter |
| **JSON Output** | Inconsistent | Deterministic (API-ready) |
| **Nuanced Intent** | Misses subtle queries | Expert-level understanding |

---

## TL;DR - Which Model to Use?

### Open-Source Mode (Groq - Recommended)
```bash
# Chat with farmers (user-facing) - GOLD STANDARD
llama-3.3-70b-versatile  # â­ Best Azerbaijani quality, 70B class

# Internal calculations (hidden)
qwen3-32b  # â­ Best math/logic

# Future upgrade path
llama-4-maverick  # When available via Groq
```

### Proprietary Mode (Gemini - Fallback Only)
```bash
# âš ï¸ Only use when open-source unavailable
gemini-2.0-flash-exp  # Cannot self-host, vendor lock-in
```

---

## Current Rankings (2026)

### For Azerbaijani Language Quality
1. ðŸ¥‡ **llama-3.3-70b-versatile** (Groq) - Best multilingual balance
2. ðŸ¥ˆ **llama-3.1-8b-instant** (Groq) - Fast, decent quality
3. ðŸ¥‰ **mixtral-8x7b-32768** (Groq) - Good alternative
4. âš ï¸ **qwen3-32b** (Groq) - Math great, but Turkish leakage risk
5. âš ï¸ **gemini-2.0-flash-exp** - Proprietary, cannot self-host

### For Math & Logic
1. ðŸ¥‡ **qwen3-32b** (Groq) - Superior calculations
2. ðŸ¥ˆ **llama-3.3-70b-versatile** (Groq) - Good all-around
3. ðŸ¥‰ **gemini-2.0-flash-exp** - Proprietary fallback

---

## Decision Tree

```
Are you deploying to production with real farmers?
â”‚
â”œâ”€ YES â†’ Use Groq (open-source models)
â”‚   â”‚
â”‚   â”œâ”€ User will see this output?
â”‚   â”‚   â”œâ”€ YES â†’ llama-3.3-70b-versatile
â”‚   â”‚   â””â”€ NO (internal calculation) â†’ qwen3-32b
â”‚   â”‚
â”‚   â””â”€ Need self-hosting for gov compliance?
â”‚       â””â”€ YES â†’ Deploy vLLM/TGI with same models
â”‚
â””â”€ NO (development/testing)
    â”‚
    â””â”€ Use Groq API (free tier: 14,400 req/day)
        â”‚
        â”œâ”€ User will see this output?
        â”‚   â””â”€ YES â†’ llama-3.3-70b-versatile
        â”‚
        â””â”€ NO (internal calculation)
            â””â”€ qwen3-32b, then rewrite with Llama
```

---

## Testing Commands

### Test Groq (Open-Source Models)
```powershell
# Test Llama for language quality
$env:YONCA_LLM_PROVIDER = "groq"
$env:YONCA_GROQ_MODEL = "llama-3.3-70b-versatile"
$env:YONCA_GROQ_API_KEY = "gsk_your_key_here"

# Start API and test
# Should respond in pure Azerbaijani, no Turkish words
```

### Test Self-Hosted (Production)
```powershell
# Point to your vLLM/TGI server
$env:YONCA_LLM_PROVIDER = "groq"
$env:YONCA_GROQ_BASE_URL = "http://your-llm-cluster:8000"
$env:YONCA_GROQ_MODEL = "meta-llama/Llama-3.3-70B-Instruct"

# Same API, same code - just local
```

---

## Configuration Examples

### Recommended Production Config (.env)
```bash
# Open-Source Mode (Recommended)
YONCA_DEPLOYMENT_MODE=open_source
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.3-70b-versatile
YONCA_GROQ_API_KEY=gsk_your_key_here

# For self-hosted production:
# YONCA_GROQ_BASE_URL=http://your-llm-cluster:8000
```

### Development Config (Groq free tier)
```bash
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.3-70b-versatile
YONCA_GROQ_API_KEY=gsk_your_key_here
# Free tier: 14,400 requests/day
```

### Fast Testing Config
```bash
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.1-8b-instant  # Fast but lower quality
YONCA_GROQ_API_KEY=gsk_your_key_here
```

---

## Common Mistakes to Avoid

### âŒ Don't Do This
```python
# Using Qwen for farmer-facing chat
messages = [
    LLMMessage.system(SYSTEM_PROMPT),
    LLMMessage.user("BuÄŸda nÉ™ vaxt É™kilir?")
]
response = qwen.generate(messages)  # âŒ May contain Turkish words!
return response.content  # âŒ Showing to farmer directly
```

### âœ… Do This Instead
```python
# Option 1: Use Llama for chat
response = llama.generate(messages)  # âœ… Better Azerbaijani
return response.content

# Option 2: Calculate with Qwen, rewrite with Llama
calculation = qwen.generate(calc_messages)  # Internal, hidden
final = llama.generate([
    LLMMessage.system("Rewrite this in perfect Azerbaijani"),
    LLMMessage.user(calculation.content)
])  # âœ… Clean output
return final.content
```

---

## Groq Model Availability (January 2026)

| Model | Speed | Quality | Azerbaijani | Math | Context |
|:------|:------|:--------|:------------|:-----|:--------|
| llama-3.3-70b-versatile | âš¡âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | 128k |
| llama-3.1-8b-instant | âš¡âš¡âš¡âš¡ | â­â­â­ | â­â­â­ | â­â­â­ | 8k |
| qwen3-32b | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­ | â­â­ âš ï¸ | â­â­â­â­â­ | 32k |
| mixtral-8x7b-32768 | âš¡âš¡âš¡ | â­â­â­â­ | â­â­â­ | â­â­â­ | 32k |

âš ï¸ = Turkish leakage risk

---

## Performance Benchmarks

```
Typical Response Times (tested January 2026):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Groq (llama-3.3-70b-versatile):
  â€¢ 500 token response: ~2.5 seconds
  â€¢ Tokens/second: 200-250

Groq (qwen3-32b):
  â€¢ 500 token response: ~1.8 seconds
  â€¢ Tokens/second: 250-300

Ollama (atllama) on CPU (i7-12th gen):
  â€¢ 500 token response: ~45 seconds
  â€¢ Tokens/second: 10-12

Ollama (atllama) on GPU (RTX 4060):
  â€¢ 500 token response: ~8 seconds
  â€¢ Tokens/second: 60-70
```

---

## Next Steps

1. **Update your .env:**
   ```bash
   YONCA_LLM_PROVIDER=groq
   YONCA_GROQ_MODEL=llama-3.3-70b-versatile
   YONCA_GROQ_API_KEY=gsk_your_key_here
   ```

2. **Test the model:**
   ```bash
   # Restart API server
   # Send test message: "BuÄŸda É™kmÉ™k Ã¼Ã§Ã¼n É™n yaxÅŸÄ± vaxt nÉ™dir?"
   # Verify: No "eylÃ¼l", "zemin", or other Turkish words
   ```

3. **When building LangGraph:**
   - Use `get_model_for_node()` helper from `model_roles.py`
   - Reasoning nodes â†’ Qwen (math/logic)
   - Chat nodes â†’ Llama (Azerbaijani quality)

4. **For production (government compliance):**
   - Deploy vLLM or TGI on-premises
   - Point `YONCA_GROQ_BASE_URL` to your cluster
   - Same models, full data control

---

## Why Open-Source?

| Benefit | Description |
|:--------|:------------|
| **Self-Hosting** | Deploy same models on your own infrastructure |
| **No Vendor Lock-in** | Switch providers anytime, no code changes |
| **Data Privacy** | Keep all data on-premises for government compliance |
| **Cost Control** | One-time hardware investment vs per-token pricing |
| **Customization** | Fine-tune models on Azerbaijani agricultural data |

---

## Hardware Quick Reference

### For 70B Gold Standard Models

| Option | Hardware | VRAM | Cost | Performance |
|:-------|:---------|:-----|:-----|:------------|
| **Workstation** | 2Ã— RTX 5090 | 64GB | ~$6,500 | 15-20 tok/s |
| **Mac Studio** | M3/M4 Ultra | 128GB Unified | ~$4,800 | 10-15 tok/s |
| **AzInCloud** | NVIDIA H100 | 80GB | â‚¬2.80/hr | 50-100 tok/s |

> ðŸ’¡ **Break-even vs cloud APIs: ~5 months** â€” See [15-HARDWARE-JUSTIFICATION.md](15-HARDWARE-JUSTIFICATION.md) for full economics.

---

## Support

Questions? Check:
- **Hardware Economics**: [15-HARDWARE-JUSTIFICATION.md](15-HARDWARE-JUSTIFICATION.md)
- Full guide: [LANGUAGE-INTERFERENCE-GUIDE.md](LANGUAGE-INTERFERENCE-GUIDE.md)
- Deployment: [12-DUAL-MODE-DEPLOYMENT.md](12-DUAL-MODE-DEPLOYMENT.md)
- Model config: [src/yonca/llm/model_roles.py](../../src/yonca/llm/model_roles.py)
- System prompts: [prompts/system/](../../prompts/system/)
