# Quick Reference: Model Selection for Yonca AI

## TL;DR - Which Model to Use?

### Open-Source Mode (Groq - Recommended)
```bash
# Chat with farmers (user-facing)
llama-3.3-70b-versatile  # ‚≠ê Best Azerbaijani quality

# Internal calculations (hidden)
qwen3-32b  # ‚≠ê Best math/logic
```

### Proprietary Mode (Gemini - Fallback Only)
```bash
# ‚ö†Ô∏è Only use when open-source unavailable
gemini-2.0-flash-exp  # Cannot self-host, vendor lock-in
```

---

## Current Rankings (2026)

### For Azerbaijani Language Quality
1. ü•á **llama-3.3-70b-versatile** (Groq) - Best multilingual balance
2. ü•à **llama-3.1-8b-instant** (Groq) - Fast, decent quality
3. ü•â **mixtral-8x7b-32768** (Groq) - Good alternative
4. ‚ö†Ô∏è **qwen3-32b** (Groq) - Math great, but Turkish leakage risk
5. ‚ö†Ô∏è **gemini-2.0-flash-exp** - Proprietary, cannot self-host

### For Math & Logic
1. ü•á **qwen3-32b** (Groq) - Superior calculations
2. ü•à **llama-3.3-70b-versatile** (Groq) - Good all-around
3. ü•â **gemini-2.0-flash-exp** - Proprietary fallback

---

## Decision Tree

```
Are you deploying to production with real farmers?
‚îÇ
‚îú‚îÄ YES ‚Üí Use Groq (open-source models)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ User will see this output?
‚îÇ   ‚îÇ   ‚îú‚îÄ YES ‚Üí llama-3.3-70b-versatile
‚îÇ   ‚îÇ   ‚îî‚îÄ NO (internal calculation) ‚Üí qwen3-32b
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ Need self-hosting for gov compliance?
‚îÇ       ‚îî‚îÄ YES ‚Üí Deploy vLLM/TGI with same models
‚îÇ
‚îî‚îÄ NO (development/testing)
    ‚îÇ
    ‚îî‚îÄ Use Groq API (free tier: 14,400 req/day)
        ‚îÇ
        ‚îú‚îÄ User will see this output?
        ‚îÇ   ‚îî‚îÄ YES ‚Üí llama-3.3-70b-versatile
        ‚îÇ
        ‚îî‚îÄ NO (internal calculation)
            ‚îî‚îÄ qwen3-32b, then rewrite with Llama
```

---

## Testing Commands

### Test Groq (Open-Source Models)
```powershell
# Test Llama for language quality
$env:YONCA_LLM_PROVIDER = "groq"
$env:YONCA_GROQ_MODEL = "llama-3.3-70b-versatile"
$env:YONCA_GROQ_API_KEY = "gsk_your_key_here"

# Start API and test
# Should respond in pure Azerbaijani, no Turkish words
```

### Test Self-Hosted (Production)
```powershell
# Point to your vLLM/TGI server
$env:YONCA_LLM_PROVIDER = "groq"
$env:YONCA_GROQ_BASE_URL = "http://your-llm-cluster:8000"
$env:YONCA_GROQ_MODEL = "meta-llama/Llama-3.3-70B-Instruct"

# Same API, same code - just local
```

---

## Configuration Examples

### Recommended Production Config (.env)
```bash
# Open-Source Mode (Recommended)
YONCA_DEPLOYMENT_MODE=open_source
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.3-70b-versatile
YONCA_GROQ_API_KEY=gsk_your_key_here

# For self-hosted production:
# YONCA_GROQ_BASE_URL=http://your-llm-cluster:8000
```

### Development Config (Groq free tier)
```bash
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.3-70b-versatile
YONCA_GROQ_API_KEY=gsk_your_key_here
# Free tier: 14,400 requests/day
```

### Fast Testing Config
```bash
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.1-8b-instant  # Fast but lower quality
YONCA_GROQ_API_KEY=gsk_your_key_here
```

---

## Common Mistakes to Avoid

### ‚ùå Don't Do This
```python
# Using Qwen for farmer-facing chat
messages = [
    LLMMessage.system(SYSTEM_PROMPT),
    LLMMessage.user("Buƒüda n…ô vaxt …ôkilir?")
]
response = qwen.generate(messages)  # ‚ùå May contain Turkish words!
return response.content  # ‚ùå Showing to farmer directly
```

### ‚úÖ Do This Instead
```python
# Option 1: Use Llama for chat
response = llama.generate(messages)  # ‚úÖ Better Azerbaijani
return response.content

# Option 2: Calculate with Qwen, rewrite with Llama
calculation = qwen.generate(calc_messages)  # Internal, hidden
final = llama.generate([
    LLMMessage.system("Rewrite this in perfect Azerbaijani"),
    LLMMessage.user(calculation.content)
])  # ‚úÖ Clean output
return final.content
```

---

## Groq Model Availability (January 2026)

| Model | Speed | Quality | Azerbaijani | Math | Context |
|:------|:------|:--------|:------------|:-----|:--------|
| llama-3.3-70b-versatile | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 128k |
| llama-3.1-8b-instant | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 8k |
| qwen3-32b | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê ‚ö†Ô∏è | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 32k |
| mixtral-8x7b-32768 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 32k |

‚ö†Ô∏è = Turkish leakage risk

---

## Performance Benchmarks

```
Typical Response Times (tested January 2026):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Groq (llama-3.3-70b-versatile):
  ‚Ä¢ 500 token response: ~2.5 seconds
  ‚Ä¢ Tokens/second: 200-250

Groq (qwen3-32b):
  ‚Ä¢ 500 token response: ~1.8 seconds
  ‚Ä¢ Tokens/second: 250-300

Ollama (atllama) on CPU (i7-12th gen):
  ‚Ä¢ 500 token response: ~45 seconds
  ‚Ä¢ Tokens/second: 10-12

Ollama (atllama) on GPU (RTX 4060):
  ‚Ä¢ 500 token response: ~8 seconds
  ‚Ä¢ Tokens/second: 60-70
```

---

## Next Steps

1. **Update your .env:**
   ```bash
   YONCA_LLM_PROVIDER=groq
   YONCA_GROQ_MODEL=llama-3.3-70b-versatile
   YONCA_GROQ_API_KEY=gsk_your_key_here
   ```

2. **Test the model:**
   ```bash
   # Restart API server
   # Send test message: "Buƒüda …ôkm…ôk √º√ß√ºn …ôn yax≈üƒ± vaxt n…ôdir?"
   # Verify: No "eyl√ºl", "zemin", or other Turkish words
   ```

3. **When building LangGraph:**
   - Use `get_model_for_node()` helper from `model_roles.py`
   - Reasoning nodes ‚Üí Qwen (math/logic)
   - Chat nodes ‚Üí Llama (Azerbaijani quality)

4. **For production (government compliance):**
   - Deploy vLLM or TGI on-premises
   - Point `YONCA_GROQ_BASE_URL` to your cluster
   - Same models, full data control

---

## Why Open-Source?

| Benefit | Description |
|:--------|:------------|
| **Self-Hosting** | Deploy same models on your own infrastructure |
| **No Vendor Lock-in** | Switch providers anytime, no code changes |
| **Data Privacy** | Keep all data on-premises for government compliance |
| **Cost Control** | One-time hardware investment vs per-token pricing |
| **Customization** | Fine-tune models on Azerbaijani agricultural data |

---

## Support

Questions? Check:
- Full guide: [LANGUAGE-INTERFERENCE-GUIDE.md](LANGUAGE-INTERFERENCE-GUIDE.md)
- Deployment: [12-DUAL-MODE-DEPLOYMENT.md](12-DUAL-MODE-DEPLOYMENT.md)
- Model config: [src/yonca/llm/model_roles.py](../../src/yonca/llm/model_roles.py)
- System prompts: [prompts/system/](../../prompts/system/)
