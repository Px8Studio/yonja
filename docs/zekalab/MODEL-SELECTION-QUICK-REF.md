# Quick Reference: Model Selection for Yonca AI

## TL;DR - Which Model to Use?

### Cloud Mode (Groq)
```bash
# Chat with farmers (user-facing)
llama-3.3-70b-versatile  # â­ Best Azerbaijani quality

# Internal calculations (hidden)
qwen3-32b  # â­ Best math/logic
```

### Local Mode (Ollama)
```bash
# Chat with farmers
atllama  # â­ Fine-tuned for Azerbaijani, no Turkish leakage

# Internal calculations
qwen3:4b  # Good math, but rewrite output with atllama
```

---

## Current Rankings (2026)

### For Azerbaijani Language Quality
1. ğŸ¥‡ **atllama** (local) - Fine-tuned, zero Turkish leakage
2. ğŸ¥ˆ **llama-3.3-70b-versatile** (Groq) - Balanced multilingual
3. ğŸ¥‰ **llama-3.1-8b-instant** (Groq) - Fast, decent quality
4. âš ï¸ **qwen3-32b** (Groq) - Math great, language has Turkish leakage
5. âš ï¸ **qwen3:4b** (local) - Same as above, smaller

### For Math & Logic
1. ğŸ¥‡ **qwen3-32b** (Groq) - Superior calculations
2. ğŸ¥ˆ **qwen3:4b** (local) - Good for local
3. ğŸ¥‰ **llama-3.3-70b-versatile** (Groq) - Decent
4. **atllama** (local) - Basic math only

---

## Decision Tree

```
Are you deploying to production with real farmers?
â”‚
â”œâ”€ YES â†’ Use Groq (cloud)
â”‚   â”‚
â”‚   â”œâ”€ User will see this output?
â”‚   â”‚   â”œâ”€ YES â†’ llama-3.3-70b-versatile
â”‚   â”‚   â””â”€ NO (internal calculation) â†’ qwen3-32b
â”‚   â”‚
â”‚   â””â”€ Fallback if Groq down â†’ atllama (local)
â”‚
â””â”€ NO (development/testing/offline)
    â”‚
    â””â”€ Use Ollama (local)
        â”‚
        â”œâ”€ User will see this output?
        â”‚   â””â”€ YES â†’ atllama (always!)
        â”‚
        â””â”€ NO (internal calculation)
            â””â”€ qwen3:4b, then rewrite with atllama
```

---

## Testing Commands

### Test Groq (Cloud)
```powershell
# Test Llama for language quality
$env:YONCA_LLM_PROVIDER = "groq"
$env:YONCA_GROQ_MODEL = "llama-3.3-70b-versatile"
$env:YONCA_GROQ_API_KEY = "gsk_your_key_here"

# Start API and test
# Should respond in pure Azerbaijani, no Turkish words
```

### Test Ollama (Local)
```powershell
# Test ATLLaMA for offline quality
$env:YONCA_LLM_PROVIDER = "ollama"
$env:YONCA_OLLAMA_MODEL = "atllama"

# Start API and test
# Slower, but best Azerbaijani quality
```

---

## Configuration Examples

### Recommended Production Config (.env)
```bash
# Primary provider (cloud for speed)
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.3-70b-versatile  # Changed from llama-3.1-8b-instant
YONCA_GROQ_API_KEY=gsk_your_key_here

# Fallback provider (local for offline)
YONCA_OLLAMA_BASE_URL=http://localhost:11434
YONCA_OLLAMA_MODEL=atllama  # Changed from qwen3:4b
```

### Development Config (local only)
```bash
YONCA_LLM_PROVIDER=ollama
YONCA_OLLAMA_MODEL=atllama
YONCA_OLLAMA_BASE_URL=http://localhost:11434
```

### Testing Config (fastest)
```bash
YONCA_LLM_PROVIDER=groq
YONCA_GROQ_MODEL=llama-3.1-8b-instant  # Fast but lower quality
YONCA_GROQ_API_KEY=gsk_your_key_here
```

---

## Common Mistakes to Avoid

### âŒ Don't Do This
```python
# Using Qwen for farmer-facing chat
messages = [
    LLMMessage.system(SYSTEM_PROMPT),
    LLMMessage.user("BuÄŸda nÉ™ vaxt É™kilir?")
]
response = qwen.generate(messages)  # âŒ May contain Turkish words!
return response.content  # âŒ Showing to farmer directly
```

### âœ… Do This Instead
```python
# Option 1: Use Llama for chat
response = llama.generate(messages)  # âœ… Better Azerbaijani
return response.content

# Option 2: Calculate with Qwen, rewrite with Llama
calculation = qwen.generate(calc_messages)  # Internal, hidden
final = llama.generate([
    LLMMessage.system("Rewrite this in perfect Azerbaijani"),
    LLMMessage.user(calculation.content)
])  # âœ… Clean output
return final.content
```

---

## Groq Model Availability (January 2026)

| Model | Speed | Quality | Azerbaijani | Math | Context |
|:------|:------|:--------|:------------|:-----|:--------|
| llama-3.3-70b-versatile | âš¡âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | 128k |
| llama-3.1-8b-instant | âš¡âš¡âš¡âš¡ | â­â­â­ | â­â­â­ | â­â­â­ | 8k |
| qwen3-32b | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­ | â­â­ âš ï¸ | â­â­â­â­â­ | 32k |
| mixtral-8x7b-32768 | âš¡âš¡âš¡ | â­â­â­â­ | â­â­â­ | â­â­â­ | 32k |

âš ï¸ = Turkish leakage risk

---

## Performance Benchmarks

```
Typical Response Times (tested January 2026):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Groq (llama-3.3-70b-versatile):
  â€¢ 500 token response: ~2.5 seconds
  â€¢ Tokens/second: 200-250

Groq (qwen3-32b):
  â€¢ 500 token response: ~1.8 seconds
  â€¢ Tokens/second: 250-300

Ollama (atllama) on CPU (i7-12th gen):
  â€¢ 500 token response: ~45 seconds
  â€¢ Tokens/second: 10-12

Ollama (atllama) on GPU (RTX 4060):
  â€¢ 500 token response: ~8 seconds
  â€¢ Tokens/second: 60-70
```

---

## Next Steps

1. **Update your .env:**
   ```bash
   YONCA_GROQ_MODEL=llama-3.3-70b-versatile
   ```

2. **Test the new model:**
   ```bash
   # Restart API server
   # Send test message: "BuÄŸda É™kmÉ™k Ã¼Ã§Ã¼n É™n yaxÅŸÄ± vaxt nÉ™dir?"
   # Verify: No "eylÃ¼l", "zemin", or other Turkish words
   ```

3. **When building LangGraph:**
   - Use `get_model_for_node()` helper from `model_roles.py`
   - Reasoning nodes â†’ Qwen
   - Chat nodes â†’ Llama/ATLLaMA

4. **Monitor production:**
   - Log all responses
   - Flag Turkish word occurrences
   - Collect farmer feedback on language quality

---

## Support

Questions? Check:
- Full guide: [LANGUAGE-INTERFERENCE-GUIDE.md](LANGUAGE-INTERFERENCE-GUIDE.md)
- Model config: [src/yonca/llm/model_roles.py](../src/yonca/llm/model_roles.py)
- System prompts: [prompts/system/](../prompts/system/)
