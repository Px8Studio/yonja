#!/usr/bin/env python3
"""Test Maverick integration in Yonca AI."""

import asyncio
import os
import sys

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from yonca.llm.model_roles import (
    get_model_for_node,
    MODEL_ROLES,
    LANGGRAPH_NODE_MODELS,
    should_rewrite_response,
)
from yonca.config import settings


def test_model_roles():
    """Test model role configuration."""
    print("=" * 60)
    print("üß™ MAVERICK INTEGRATION TEST")
    print("=" * 60)
    
    # Test 1: Maverick is in MODEL_ROLES
    print("\nüìã Test 1: Maverick in MODEL_ROLES")
    maverick_key = "meta-llama/llama-4-maverick-17b-128e-instruct"
    if maverick_key in MODEL_ROLES:
        maverick = MODEL_ROLES[maverick_key]
        print(f"   ‚úÖ Found Maverick")
        print(f"   Role: {maverick.get('role')}")
        print(f"   Multimodal: {maverick.get('multimodal')}")
        print(f"   Azerbaijani Quality: {maverick.get('azerbaijani_quality')}")
    else:
        print(f"   ‚ùå Maverick NOT found in MODEL_ROLES")
        return False
    
    # Test 2: get_model_for_node defaults to maverick
    print("\nüìã Test 2: get_model_for_node() defaults")
    nodes = ["supervisor", "response_writer", "irrigation_calculator"]
    for node in nodes:
        model = get_model_for_node(node)  # Should default to maverick
        print(f"   {node}: {model}")
        if "maverick" not in model:
            print(f"   ‚ö†Ô∏è Expected maverick, got {model}")
    
    # Test 3: Legacy mode still works
    print("\nüìã Test 3: Legacy mode (open_source)")
    legacy_model = get_model_for_node("response_writer", "open_source")
    print(f"   response_writer (legacy): {legacy_model}")
    if legacy_model == "llama-3.3-70b-versatile":
        print("   ‚úÖ Legacy mode works")
    else:
        print(f"   ‚ö†Ô∏è Expected llama-3.3-70b-versatile, got {legacy_model}")
    
    # Test 4: should_rewrite_response
    print("\nüìã Test 4: should_rewrite_response()")
    test_cases = [
        ("meta-llama/llama-4-maverick-17b-128e-instruct", False),
        ("qwen3-32b", True),
        ("llama-3.3-70b-versatile", False),
    ]
    for model, expected in test_cases:
        result = should_rewrite_response(model)
        status = "‚úÖ" if result == expected else "‚ùå"
        print(f"   {status} {model}: {result} (expected {expected})")
    
    # Test 5: LANGGRAPH_NODE_MODELS has maverick mode
    print("\nüìã Test 5: LANGGRAPH_NODE_MODELS modes")
    modes = list(LANGGRAPH_NODE_MODELS.keys())
    print(f"   Available modes: {modes}")
    if "maverick" in modes:
        print("   ‚úÖ Maverick mode available")
    else:
        print("   ‚ùå Maverick mode NOT found")
        return False
    
    # Test 6: Config default
    print("\nüìã Test 6: Config settings")
    print(f"   Default groq_model: {settings.groq_model}")
    if "maverick" in settings.groq_model:
        print("   ‚úÖ Config defaults to Maverick")
    else:
        print(f"   ‚ö†Ô∏è Config still using: {settings.groq_model}")
    
    print("\n" + "=" * 60)
    print("‚úÖ All model role tests passed!")
    print("=" * 60)
    return True


async def test_groq_provider():
    """Test Groq provider with Maverick (requires API key)."""
    print("\n" + "=" * 60)
    print("üöÄ GROQ PROVIDER TEST")
    print("=" * 60)
    
    if not settings.groq_api_key:
        print("‚ö†Ô∏è No GROQ_API_KEY set. Skipping provider test.")
        print("   Set YONCA_GROQ_API_KEY in .env or environment")
        return
    
    from yonca.llm.factory import create_groq_provider
    from yonca.llm.providers.base import LLMMessage
    
    # Create Maverick provider
    provider = create_groq_provider(model="meta-llama/llama-4-maverick-17b-128e-instruct")
    print(f"\nüìã Provider: {provider.provider_name}")
    print(f"   Model: {provider.model_name}")
    
    # Health check
    print("\nüìã Health Check...")
    is_healthy = await provider.health_check()
    if is_healthy:
        print("   ‚úÖ Groq API is healthy")
    else:
        print("   ‚ùå Groq API health check failed")
        return
    
    # Test Azerbaijani response
    print("\nüìã Testing Azerbaijani Response...")
    messages = [
        LLMMessage.system(
            "S…ôn Yonca adlƒ± Az…ôrbaycan k…ônd t…ôs…ôrr√ºfatƒ± AI k√∂m…ôk√ßisis…ôn. "
            "YALNIZ Az…ôrbaycan dilind…ô cavab ver. T√ºrkc…ô s√∂zl…ôr istifad…ô etm…ô."
        ),
        LLMMessage.user("Buƒüda …ôkm…ôk √º√ß√ºn …ôn yax≈üƒ± vaxt n…ôdir?")
    ]
    
    response = await provider.generate(messages, temperature=0.7, max_tokens=300)
    print(f"\n   Response ({response.tokens_used} tokens):")
    print("-" * 40)
    print(response.content[:500])
    print("-" * 40)
    
    # Check for Turkish leakage
    turkish_words = ["eyl√ºl", "zemin", "tohum", "√ºr√ºn", "sulama", "toprak"]
    found_turkish = [w for w in turkish_words if w.lower() in response.content.lower()]
    if found_turkish:
        print(f"   ‚ö†Ô∏è Possible Turkish leakage: {found_turkish}")
    else:
        print("   ‚úÖ No Turkish leakage detected")
    
    print("\n" + "=" * 60)
    print("‚úÖ Groq provider test complete!")
    print("=" * 60)


def main():
    """Run all tests."""
    # Test model roles (no API needed)
    if not test_model_roles():
        sys.exit(1)
    
    # Test Groq provider (needs API key)
    asyncio.run(test_groq_provider())


if __name__ == "__main__":
    main()
